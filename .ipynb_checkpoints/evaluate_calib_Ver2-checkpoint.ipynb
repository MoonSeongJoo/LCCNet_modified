{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "governmental-browser",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "cannot use vispy, setting triangulate_corr as None\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import open3d as o3\n",
    "\n",
    "import cv2\n",
    "import mathutils\n",
    "# import matplotlib\n",
    "# matplotlib.use('Qt5Agg')\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torchvision.transforms import functional as tvtf\n",
    "from sacred import Experiment\n",
    "from sacred.utils import apply_backspaces_and_linefeeds\n",
    "from skimage import io\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "#from models.LCCNet import LCCNet\n",
    "\n",
    "from LCCNet_COTR_moon_Ver2 import LCCNet\n",
    "\n",
    "from quaternion_distances import quaternion_distance\n",
    "from utils import (mat2xyzrpy, merge_inputs, overlay_imgs, quat2mat,\n",
    "                   quaternion_from_matrix, rotate_back, rotate_forward,\n",
    "                   tvector2mat)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from pykitti import odometry\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from math import radians\n",
    "from utils import invert_pose\n",
    "from torchvision import transforms\n",
    "#sacred read-only function error correct!!\n",
    "from sacred import SETTINGS \n",
    "SETTINGS.CONFIG.READ_ONLY_CONFIG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "approved-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTest(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_dir, transform=None, augmentation=False, use_reflectance=False,\n",
    "                 max_t=1.5, max_r=20., split='random', device='cpu', test_sequence='00', est='rot'):\n",
    "        super(DatasetTest, self).__init__()\n",
    "        self.use_reflectance = use_reflectance\n",
    "        self.maps_folder = ''\n",
    "        self.device = device\n",
    "        self.max_r = max_r\n",
    "        self.max_t = max_t\n",
    "        self.augmentation = augmentation\n",
    "        self.root_dir = dataset_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.GTs_R = {}\n",
    "        self.GTs_T = {}\n",
    "        self.GTs_T_cam02_velo = {}\n",
    "        self.K = {}\n",
    "\n",
    "        self.all_files = []\n",
    "\n",
    "        self.calib_path =\"data_odometry_calib\"\n",
    "        self.image_path =\"data_odometry_color\"\n",
    "        self.velodyne_path = \"data_odometry_velodyne\"\n",
    "        self.imagegray_path = \"data_odometry_gray\"\n",
    "        self.poses_path = \"data_odometry_poses\"\n",
    "        self.val_RT_path = \"data_odometry_valRT\"\n",
    "        self.test_RT_path = \"data_odometry_testRT\"\n",
    "        \n",
    "        self.calib_path_total = os.path.join(dataset_dir,self.calib_path,\"dataset\")\n",
    "        self.image_path_total = os.path.join(dataset_dir,self.image_path,\"dataset\")\n",
    "        self.imagegray_path_total = os.path.join(dataset_dir,self.imagegray_path,\"dataset\")\n",
    "        self.velodyne_path_total = os.path.join(dataset_dir,self.velodyne_path,\"dataset\")\n",
    "        self.poses_path_total = os.path.join(dataset_dir,self.poses_path,\"dataset\",\"poses\")\n",
    "        self.val_RT_path_total = os.path.join(dataset_dir,self.val_RT_path,\"dataset\")\n",
    "        self.test_RT_path_total = os.path.join(dataset_dir,self.test_RT_path,\"dataset\")\n",
    "\n",
    "        seq = test_sequence\n",
    "        odom = odometry(self.calib_path_total, self.poses_path_total, seq)\n",
    "        calib = odom.calib\n",
    "        T_cam02_velo_np = calib.T_cam2_velo #gt pose from cam02 to velo_lidar (T_cam02_velo: 4x4)\n",
    "        self.K[seq] = calib.K_cam2 # 3x3\n",
    "        T_cam02_velo = torch.from_numpy(T_cam02_velo_np)\n",
    "        GT_R = quaternion_from_matrix(T_cam02_velo[:3, :3])\n",
    "        GT_T = T_cam02_velo[3:, :3]\n",
    "        self.GTs_R[seq] = GT_R # GT_R = np.array([row['qw'], row['qx'], row['qy'], row['qz']])\n",
    "        self.GTs_T[seq] = GT_T # GT_T = np.array([row['x'], row['y'], row['z']])\n",
    "        self.GTs_T_cam02_velo[seq] = T_cam02_velo_np #gt pose from cam02 to velo_lidar (T_cam02_velo: 4x4)\n",
    "\n",
    "        image_list = os.listdir(os.path.join(self.image_path_total, 'sequences', seq, 'image_2'))\n",
    "        image_list.sort()\n",
    "\n",
    "        for image_name in image_list:\n",
    "            if not os.path.exists(os.path.join(self.velodyne_path_total, 'sequences', seq, 'velodyne',\n",
    "                                               str(image_name.split('.')[0])+'.bin')):\n",
    "                continue\n",
    "            if not os.path.exists(os.path.join(self.image_path_total, 'sequences', seq, 'image_2',\n",
    "                                               str(image_name.split('.')[0])+'.png')):\n",
    "                continue\n",
    "            self.all_files.append(os.path.join(seq, image_name.split('.')[0]))\n",
    "\n",
    "        self.test_RT = []\n",
    "        if split == 'random':\n",
    "            test_RT_sequences_path = os.path.join(self.test_RT_path_total,\"sequences\")\n",
    "            test_RT_file = os.path.join(self.test_RT_path_total, 'sequences',\n",
    "                                       f'test_RT_seq{test_sequence}_{max_r:.2f}_{max_t:.2f}.csv')\n",
    "            \n",
    "            if not os.path.exists(test_RT_sequences_path):\n",
    "                os.makedirs(test_RT_sequences_path)\n",
    "            if os.path.exists(test_RT_file):\n",
    "                print(f'TEST SET: Using this file: {test_RT_file}')\n",
    "                df_test_RT = pd.read_csv(test_RT_file, sep=',')\n",
    "                for index, row in df_test_RT.iterrows():\n",
    "                    self.test_RT.append(list(row))\n",
    "            else:\n",
    "                print(f'TEST SET - Not found: {test_RT_file}')\n",
    "                print(\"Generating a new one\")\n",
    "                test_RT_file = open(test_RT_file, 'w')\n",
    "                test_RT_file = csv.writer(test_RT_file, delimiter=',')\n",
    "                test_RT_file.writerow(['id', 'tx', 'ty', 'tz', 'rx', 'ry', 'rz'])\n",
    "                for i in range(len(self.all_files)):\n",
    "                    rotz = np.random.uniform(-max_r, max_r) * (3.141592 / 180.0)\n",
    "                    roty = np.random.uniform(-max_r, max_r) * (3.141592 / 180.0)\n",
    "                    rotx = np.random.uniform(-max_r, max_r) * (3.141592 / 180.0)\n",
    "                    transl_x = np.random.uniform(-max_t, max_t)\n",
    "                    transl_y = np.random.uniform(-max_t, max_t)\n",
    "                    # transl_z = np.random.uniform(-max_t, max_t)\n",
    "                    transl_z = np.random.uniform(-max_t, min(max_t, 1.))\n",
    "                    test_RT_file.writerow([i, transl_x, transl_y, transl_z,\n",
    "                                           rotx, roty, rotz])\n",
    "                    self.test_RT.append([float(i), transl_x, transl_y, transl_z,\n",
    "                                         rotx, roty, rotz])\n",
    "\n",
    "            assert len(self.test_RT) == len(self.all_files), \"Something wrong with test RTs\"\n",
    "        elif split == 'constant':\n",
    "            for i in range(len(self.all_files)):\n",
    "                if est == 'rot':\n",
    "                    rotz = np.random.uniform(-max_r, max_r) * (3.141592 / 180.0)\n",
    "                    roty = np.random.uniform(-max_r, max_r) * (3.141592 / 180.0)\n",
    "                    rotx = np.random.uniform(-max_r, max_r) * (3.141592 / 180.0)\n",
    "                    transl_x = max_t\n",
    "                    transl_y = max_t\n",
    "                    transl_z = max_t\n",
    "                elif est == 'tran':\n",
    "                    rotz = max_r * (3.141592 / 180.0)\n",
    "                    roty = max_r * (3.141592 / 180.0)\n",
    "                    rotx = max_r * (3.141592 / 180.0)\n",
    "                    transl_x = np.random.uniform(-max_t, max_t)\n",
    "                    transl_y = np.random.uniform(-max_t, max_t)\n",
    "                    transl_z = np.random.uniform(-max_t, max_t)\n",
    "                # transl_z = np.random.uniform(-max_t, min(max_t, 1.))\n",
    "                self.test_RT.append([i, transl_x, transl_y, transl_z,\n",
    "                                     rotx, roty, rotz])\n",
    "\n",
    "    def get_ground_truth_poses(self, sequence, frame):\n",
    "        return self.GTs_T[sequence][frame], self.GTs_R[sequence][frame]\n",
    "\n",
    "    def custom_transform(self, rgb):\n",
    "        to_tensor = transforms.ToTensor()\n",
    "        normalization = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        rgb = to_tensor(rgb)\n",
    "        rgb = normalization(rgb)\n",
    "        return rgb\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.all_files[idx]\n",
    "        seq = str(item.split('/')[0])\n",
    "        rgb_name = str(item.split('/')[1])\n",
    "        img_path = os.path.join(self.image_path_total, 'sequences', seq, 'image_2', rgb_name+'.png')\n",
    "        lidar_path = os.path.join(self.velodyne_path_total, 'sequences', seq, 'velodyne', rgb_name+'.bin')\n",
    "        lidar_scan = np.fromfile(lidar_path, dtype=np.float32)\n",
    "        pc = lidar_scan.reshape((-1, 4))\n",
    "        valid_indices = pc[:, 0] < -3.\n",
    "        valid_indices = valid_indices | (pc[:, 0] > 3.)\n",
    "        valid_indices = valid_indices | (pc[:, 1] < -3.)\n",
    "        valid_indices = valid_indices | (pc[:, 1] > 3.)\n",
    "        pc = pc[valid_indices].copy()\n",
    "        if self.use_reflectance:\n",
    "            reflectance = pc[:, 3].copy()\n",
    "            reflectance = torch.from_numpy(reflectance).float()\n",
    "\n",
    "        RT = self.GTs_T_cam02_velo[seq].astype(np.float32)\n",
    "\n",
    "        pc_rot = np.matmul(RT, pc.T)\n",
    "        pc_rot = pc_rot.astype(np.float).T.copy()\n",
    "        pc_in = torch.from_numpy(pc_rot.astype(np.float32))#.float()\n",
    "\n",
    "        if pc_in.shape[1] == 4 or pc_in.shape[1] == 3:\n",
    "            pc_in = pc_in.t()\n",
    "        if pc_in.shape[0] == 3:\n",
    "            homogeneous = torch.ones(pc_in.shape[1]).unsqueeze(0)\n",
    "            pc_in = torch.cat((pc_in, homogeneous), 0)\n",
    "        elif pc_in.shape[0] == 4:\n",
    "            if not torch.all(pc_in[3,:] == 1.):\n",
    "                pc_in[3,:] = 1.\n",
    "        else:\n",
    "            raise TypeError(\"Wrong PointCloud shape\")\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        try:\n",
    "            img = self.custom_transform(img)\n",
    "        except OSError:\n",
    "            new_idx = np.random.randint(0, self.__len__())\n",
    "            return self.__getitem__(new_idx)\n",
    "\n",
    "\n",
    "        initial_RT = self.test_RT[idx]\n",
    "        rotz = initial_RT[6]\n",
    "        roty = initial_RT[5]\n",
    "        rotx = initial_RT[4]\n",
    "        transl_x = initial_RT[1]\n",
    "        transl_y = initial_RT[2]\n",
    "        transl_z = initial_RT[3]\n",
    "\n",
    "        #R = mathutils.Euler((rotx, roty, rotz), 'XYZ')\n",
    "        R = mathutils.Euler((rotx, roty, rotz))\n",
    "        T = mathutils.Vector((transl_x, transl_y, transl_z))\n",
    "\n",
    "        R, T = invert_pose(R, T)\n",
    "        R, T = torch.tensor(R), torch.tensor(T)\n",
    "        calib = self.K[seq]\n",
    "\n",
    "        sample = {'rgb': img, 'point_cloud': pc_in, 'calib': calib,\n",
    "                  'tr_error': T, 'rot_error': R, 'seq': int(seq), 'img_path': img_path,\n",
    "                  'rgb_name': rgb_name + '.png', 'item': item, 'extrin': RT,\n",
    "                  'initial_RT': initial_RT}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "answering-stocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# matplotlib.rc(\"font\",family='AR PL UMing CN')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "# plt.rc('font',family='Times New Roman')\n",
    "font_EN = {'family': 'Times New Roman', 'weight': 'normal', 'size': 16}\n",
    "font_CN = {'family': 'AR PL UMing CN', 'weight': 'normal', 'size': 16}\n",
    "plt_size = 10.5\n",
    "\n",
    "ex = Experiment(\"LCCNet-evaluate-iterative\",interactive = True)\n",
    "ex.captured_out_filter = apply_backspaces_and_linefeeds\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1, 2, 3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noinspection PyUnusedLocal\n",
    "@ex.config\n",
    "def config():\n",
    "    dataset = 'kitti/odom'\n",
    "    data_folder = '/mnt/data/kitti_odometry'\n",
    "    test_sequence = 0\n",
    "    use_prev_output = False\n",
    "    max_t = 1.5\n",
    "    max_r = 20.\n",
    "    occlusion_kernel = 5\n",
    "    occlusion_threshold = 3.0\n",
    "    network = 'Res_f1'\n",
    "    norm = 'bn'\n",
    "    show =  False\n",
    "    use_reflectance = False\n",
    "    weight = None  # List of weights' path, for iterative refinement\n",
    "    #weight =  './pretrained/kitti/kitti_iter1.tar'\n",
    "    save_name = None\n",
    "    # Set to True only if you use two network, the first for rotation and the second for translation\n",
    "    rot_transl_separated = False\n",
    "    random_initial_pose = False\n",
    "    save_log = False\n",
    "    dropout = 0.0\n",
    "    max_depth = 80.\n",
    "    iterative_method = 'single' # ['multi_range', 'single_range', 'single']\n",
    "    output = './output'\n",
    "    save_image = False\n",
    "    outlier_filter = True\n",
    "    outlier_filter_th = 10\n",
    "    out_fig_lg = 'EN' # [EN, CN]\n",
    "\n",
    "weights = [\n",
    "   '/root/work/LCCNet_Moon/checkpoints/kitti/odom/val_seq_00/models/checkpoint_r10.00_t1.00_e0_3.526.tar',\n",
    "   '/root/work/LCCNet_Moon/checkpoints/kitti/odom/val_seq_00/models/checkpoint_r10.00_t1.00_e0_5.349.tar',\n",
    "   '/root/work/LCCNet_Moon/checkpoints/kitti/odom/val_seq_00/models/checkpoint_r10.00_t1.00_e0_5.349.tar',\n",
    "   '/root/work/LCCNet_Moon/checkpoints/kitti/odom/val_seq_00/models/checkpoint_r10.00_t1.00_e0_5.349.tar',\n",
    "   '/root/work/LCCNet_Moon/checkpoints/kitti/odom/val_seq_00/models/checkpoint_r10.00_t1.00_e0_5.349.tar',\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "weights = [\n",
    "   './pretrained/kitti/kitti_iter1.tar',\n",
    "   './pretrained/kitti/kitti_iter2.tar',\n",
    "   './pretrained/kitti/kitti_iter3.tar',\n",
    "   './pretrained/kitti/kitti_iter4.tar',\n",
    "   './pretrained/kitti/kitti_iter5.tar',\n",
    "]\n",
    "\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "EPOCH = 1\n",
    "\n",
    "\n",
    "def _init_fn(worker_id, seed):\n",
    "    seed = seed + worker_id + EPOCH * 100\n",
    "    print(f\"Init worker {worker_id} with seed {seed}\")\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def get_2D_lidar_projection(pcl, cam_intrinsic):\n",
    "    pcl_xyz = cam_intrinsic @ pcl.T\n",
    "    pcl_xyz = pcl_xyz.T\n",
    "    pcl_z = pcl_xyz[:, 2]\n",
    "    pcl_xyz = pcl_xyz / (pcl_xyz[:, 2, None] + 1e-10)\n",
    "    pcl_uv = pcl_xyz[:, :2]\n",
    "\n",
    "    return pcl_uv, pcl_z\n",
    "\n",
    "\n",
    "def lidar_project_depth(pc_rotated, cam_calib, img_shape):\n",
    "    pc_rotated = pc_rotated[:3, :].detach().cpu().numpy()\n",
    "    cam_intrinsic = cam_calib.numpy()\n",
    "    pcl_uv, pcl_z = get_2D_lidar_projection(pc_rotated.T, cam_intrinsic)\n",
    "    mask = (pcl_uv[:, 0] > 0) & (pcl_uv[:, 0] < img_shape[1]) & (pcl_uv[:, 1] > 0) & (\n",
    "            pcl_uv[:, 1] < img_shape[0]) & (pcl_z > 0)\n",
    "    pcl_uv = pcl_uv[mask]\n",
    "    pcl_z = pcl_z[mask]\n",
    "    pcl_uv = pcl_uv.astype(np.uint32)\n",
    "    pcl_z = pcl_z.reshape(-1, 1)\n",
    "    depth_img = np.zeros((img_shape[0], img_shape[1], 1))\n",
    "    depth_img[pcl_uv[:, 1], pcl_uv[:, 0]] = pcl_z\n",
    "    depth_img = torch.from_numpy(depth_img.astype(np.float32))\n",
    "    depth_img = depth_img.cuda()\n",
    "    depth_img = depth_img.permute(2, 0, 1)\n",
    "    pc_valid = pc_rotated.T[mask]\n",
    "\n",
    "    return depth_img, pcl_uv, pc_valid\n",
    "\n",
    "\n",
    "def two_images_side_by_side(img_a, img_b):\n",
    "    assert img_a.shape == img_b.shape, f'{img_a.shape} vs {img_b.shape}'\n",
    "    assert img_a.dtype == img_b.dtype\n",
    "    b, h, w, c = img_a.shape\n",
    "    canvas = np.zeros((b, h, 2 * w, c), dtype=img_a.cpu().numpy().dtype)\n",
    "    canvas[:, :, 0 * w:1 * w, :] = img_a.cpu().numpy()\n",
    "    canvas[:, :, 1 * w:2 * w, :] = img_b.cpu().numpy()\n",
    "    #canvas[:, :, : , 0 * w:1 * w] = img_a.cpu().numpy()\n",
    "    #canvas[:, :, : , 1 * w:2 * w] = img_b.cpu().numpy()\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "charitable-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ex.automain\n",
    "@ex.main\n",
    "def main(_config, seed):\n",
    "    global EPOCH, weights\n",
    "    if _config['weight'] is not None:\n",
    "        weights = _config['weight']\n",
    "\n",
    "    if _config['iterative_method'] == 'single':\n",
    "        weights = [weights[0]]\n",
    "\n",
    "    # dataset_class = DatasetLidarCameraKittiOdometry\n",
    "    dataset_class = DatasetTest\n",
    "    img_shape = (384, 1280)\n",
    "    input_size = (256, 512)\n",
    "\n",
    "    # split = 'test'\n",
    "    if _config['random_initial_pose']:\n",
    "        split = 'test_random'\n",
    "\n",
    "    if _config['test_sequence'] is None:\n",
    "        raise TypeError('test_sequences cannot be None')\n",
    "    else:\n",
    "        if isinstance(_config['test_sequence'], int):\n",
    "            _config['test_sequence'] = f\"{_config['test_sequence']:02d}\"\n",
    "        # dataset_val = dataset_class(_config['data_folder'], max_r=_config['max_r'], max_t=_config['max_t'],\n",
    "        #                             split=split, use_reflectance=_config['use_reflectance'],\n",
    "        #                             val_sequence=_config['test_sequence'])\n",
    "        dataset_val = dataset_class(_config['data_folder'], max_r=_config['max_r'], max_t=_config['max_t'],\n",
    "                                    split='random', use_reflectance=_config['use_reflectance'],\n",
    "                                    test_sequence=_config['test_sequence'], est='rot')\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "\n",
    "    def init_fn(x):\n",
    "        return _init_fn(x, seed)\n",
    "\n",
    "    num_worker = 6\n",
    "    batch_size = 1\n",
    "\n",
    "    TestImgLoader = torch.utils.data.DataLoader(dataset=dataset_val,\n",
    "                                                shuffle=False,\n",
    "                                                batch_size=batch_size,\n",
    "                                                num_workers=num_worker,\n",
    "                                                worker_init_fn=init_fn,\n",
    "                                                collate_fn=merge_inputs,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=False)\n",
    "\n",
    "    print(len(TestImgLoader))\n",
    "\n",
    "    models = [] # iterative model\n",
    "    for i in range(len(weights)):\n",
    "        # network choice and settings\n",
    "        if _config['network'].startswith('Res'):\n",
    "            feat = 1\n",
    "            md = 4\n",
    "            split = _config['network'].split('_')\n",
    "            for item in split[1:]:\n",
    "                if item.startswith('f'):\n",
    "                    feat = int(item[-1])\n",
    "                elif item.startswith('md'):\n",
    "                    md = int(item[2:])\n",
    "            assert 0 < feat < 7, \"Feature Number from PWC have to be between 1 and 6\"\n",
    "            assert 0 < md, \"md must be positive\"\n",
    "            model = LCCNet(input_size, use_feat_from=feat, md=md,\n",
    "                             use_reflectance=_config['use_reflectance'], dropout=_config['dropout'])\n",
    "        else:\n",
    "            raise TypeError(\"Network unknown\")\n",
    "\n",
    "        print (\"weight[i]\", weights[i])\n",
    "        checkpoint = torch.load(weights[i], map_location='cpu')\n",
    "        saved_state_dict = checkpoint['state_dict']\n",
    "        model.load_state_dict(saved_state_dict)\n",
    "        model = model.cuda()\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "\n",
    "\n",
    "    if _config['save_log']:\n",
    "        log_file = f'./results_for_paper/log_seq{_config[\"test_sequence\"]}.csv'\n",
    "        log_file = open(log_file, 'w')\n",
    "        log_file = csv.writer(log_file)\n",
    "        header = ['frame']\n",
    "        for i in range(len(weights) + 1):\n",
    "            header += [f'iter{i}_error_t', f'iter{i}_error_r', f'iter{i}_error_x', f'iter{i}_error_y',\n",
    "                       f'iter{i}_error_z', f'iter{i}_error_r', f'iter{i}_error_p', f'iter{i}_error_y']\n",
    "        log_file.writerow(header)\n",
    "\n",
    "    show = _config['show']\n",
    "    # save image to the output path\n",
    "    _config['output'] = os.path.join(_config['output'], _config['iterative_method'])\n",
    "    rgb_path = os.path.join(_config['output'], 'rgb')\n",
    "    if not os.path.exists(rgb_path):\n",
    "        os.makedirs(rgb_path)\n",
    "    depth_path = os.path.join(_config['output'], 'depth')\n",
    "    if not os.path.exists(depth_path):\n",
    "        os.makedirs(depth_path)\n",
    "    input_path = os.path.join(_config['output'], 'input')\n",
    "    if not os.path.exists(input_path):\n",
    "        os.makedirs(input_path)\n",
    "    gt_path = os.path.join(_config['output'], 'gt')\n",
    "    if not os.path.exists(gt_path):\n",
    "        os.makedirs(gt_path)\n",
    "    if _config['out_fig_lg'] == 'EN':\n",
    "        results_path = os.path.join(_config['output'], 'results_en')\n",
    "    elif _config['out_fig_lg'] == 'CN':\n",
    "        results_path = os.path.join(_config['output'], 'results_cn')\n",
    "    if not os.path.exists(results_path):\n",
    "        os.makedirs(results_path)\n",
    "    pred_path = os.path.join(_config['output'], 'pred')\n",
    "    for it in range(len(weights)):\n",
    "        if not os.path.exists(os.path.join(pred_path, 'iteration_'+str(it+1))):\n",
    "            os.makedirs(os.path.join(pred_path, 'iteration_'+str(it+1)))\n",
    "\n",
    "    # save pointcloud to the output path\n",
    "    pc_lidar_path = os.path.join(_config['output'], 'pointcloud', 'lidar')\n",
    "    if not os.path.exists(pc_lidar_path):\n",
    "        os.makedirs(pc_lidar_path)\n",
    "    pc_input_path = os.path.join(_config['output'], 'pointcloud', 'input')\n",
    "    if not os.path.exists(pc_input_path):\n",
    "        os.makedirs(pc_input_path)\n",
    "    pc_pred_path = os.path.join(_config['output'], 'pointcloud', 'pred')\n",
    "    if not os.path.exists(pc_pred_path):\n",
    "        os.makedirs(pc_pred_path)\n",
    "\n",
    "\n",
    "    errors_r = []\n",
    "    errors_t = []\n",
    "    errors_t2 = []\n",
    "    errors_xyz = []\n",
    "    errors_rpy = []\n",
    "    all_RTs = []\n",
    "    mis_calib_list = []\n",
    "    total_time = 0\n",
    "\n",
    "    prev_tr_error = None\n",
    "    prev_rot_error = None\n",
    "\n",
    "    for i in range(len(weights) + 1):\n",
    "        errors_r.append([])\n",
    "        errors_t.append([])\n",
    "        errors_t2.append([])\n",
    "        errors_rpy.append([])\n",
    "\n",
    "    for batch_idx, sample in enumerate(tqdm(TestImgLoader)):\n",
    "        N = 100 # 500\n",
    "        # if batch_idx > 200:\n",
    "        #    break\n",
    "\n",
    "        log_string = [str(batch_idx)]\n",
    "\n",
    "        lidar_input = []\n",
    "        rgb_input = []\n",
    "        lidar_gt = []\n",
    "        shape_pad_input = []\n",
    "        real_shape_input = []\n",
    "        pc_rotated_input = []\n",
    "        RTs = []\n",
    "        shape_pad = [0, 0, 0, 0]\n",
    "        outlier_filter = False\n",
    "\n",
    "        if batch_idx == 0 or not _config['use_prev_output']:\n",
    "            # 프레임을 수정하여 GT를 입력 할 수 있습니다.\n",
    "            sample['tr_error'] = sample['tr_error'].cuda()\n",
    "            sample['rot_error'] = sample['rot_error'].cuda()\n",
    "        else:\n",
    "            sample['tr_error'] = prev_tr_error\n",
    "            sample['rot_error'] = prev_rot_error\n",
    "\n",
    "        for idx in range(len(sample['rgb'])):\n",
    "            # ProjectPointCloud in RT-pose\n",
    "            real_shape = [sample['rgb'][idx].shape[1], sample['rgb'][idx].shape[2], sample['rgb'][idx].shape[0]]\n",
    "\n",
    "            sample['point_cloud'][idx] = sample['point_cloud'][idx].cuda()  # 카메라 좌표계로 변환 된 Lidar 포인트 클라우드\n",
    "            pc_lidar = sample['point_cloud'][idx].clone()\n",
    "\n",
    "            if _config['max_depth'] < 80.:\n",
    "                pc_lidar = pc_lidar[:, pc_lidar[0, :] < _config['max_depth']].clone()\n",
    "\n",
    "            depth_gt, uv_gt, pc_gt_valid = lidar_project_depth(pc_lidar, sample['calib'][idx], real_shape)  # image_shape\n",
    "            depth_gt /= _config['max_depth']\n",
    "\n",
    "            if _config['save_image']:\n",
    "                # save the Lidar pointcloud\n",
    "                #pcl_lidar = o3.PointCloud()\n",
    "                pcl_lidar = o3.geometry.PointCloud()\n",
    "                pc_lidar = pc_lidar.detach().cpu().numpy()\n",
    "                #pcl_lidar.points = o3.Vector3dVector(pc_lidar.T[:, :3])\n",
    "                pcl_lidar.points = o3.utility.Vector3dVector(pc_lidar.T[:, :3])\n",
    "\n",
    "                # o3.draw_geometries(downpcd)\n",
    "                o3.io.write_point_cloud(pc_lidar_path + '/{}.pcd'.format(batch_idx), pcl_lidar)\n",
    "\n",
    "\n",
    "            R = quat2mat(sample['rot_error'][idx])\n",
    "            T = tvector2mat(sample['tr_error'][idx])\n",
    "            RT_inv = torch.mm(T, R)\n",
    "            RT = RT_inv.clone().inverse()\n",
    "\n",
    "            pc_rotated = rotate_back(sample['point_cloud'][idx], RT_inv)  # Pc` = RT * Pc\n",
    "\n",
    "            if _config['max_depth'] < 80.:\n",
    "                pc_rotated = pc_rotated[:, pc_rotated[0, :] < _config['max_depth']].clone()\n",
    "\n",
    "            depth_img, uv_input, pc_input_valid = lidar_project_depth(pc_rotated, sample['calib'][idx], real_shape)  # image_shape\n",
    "            depth_img /= _config['max_depth']\n",
    "\n",
    "            if _config['outlier_filter'] and uv_input.shape[0] <= _config['outlier_filter_th']:\n",
    "                outlier_filter = True\n",
    "            else:\n",
    "                outlier_filter = False\n",
    "\n",
    "            if _config['save_image']:\n",
    "                # save the RGB input pointcloud\n",
    "                img = cv2.imread(sample['img_path'][0])\n",
    "                R = img[uv_input[:, 1], uv_input[:, 0], 0] / 255\n",
    "                G = img[uv_input[:, 1], uv_input[:, 0], 1] / 255\n",
    "                B = img[uv_input[:, 1], uv_input[:, 0], 2] / 255\n",
    "                pcl_input = o3.geometry.PointCloud()\n",
    "                pcl_input.points = o3.utility.Vector3dVector(pc_input_valid[:, :3])\n",
    "                pcl_input.colors = o3.utility.Vector3dVector(np.vstack((R, G, B)).T)\n",
    "\n",
    "                # o3.draw_geometries(downpcd)\n",
    "                o3.io.write_point_cloud(pc_input_path + '/{}.pcd'.format(batch_idx), pcl_input)\n",
    "\n",
    "            # PAD ONLY ON RIGHT AND BOTTOM SIDE\n",
    "            rgb = sample['rgb'][idx].cuda()\n",
    "            shape_pad = [0, 0, 0, 0]\n",
    "\n",
    "            shape_pad[3] = (img_shape[0] - rgb.shape[1])  # // 2\n",
    "            shape_pad[1] = (img_shape[1] - rgb.shape[2])  # // 2 + 1\n",
    "\n",
    "            rgb = F.pad(rgb, shape_pad)\n",
    "            depth_img = F.pad(depth_img, shape_pad)\n",
    "            depth_gt = F.pad(depth_gt, shape_pad)\n",
    "            \n",
    "            ######### network input pre-processing start #############\n",
    "            rgb = rgb.permute(1,2,0)\n",
    "            depth_img_np = depth_img.permute(1,2,0)\n",
    "            depth_gt_np = depth_gt.permute(1,2,0)\n",
    "\n",
    "            rgb_np = rgb.cpu().numpy()\n",
    "            depth_img_np = depth_img_np.cpu().numpy()\n",
    "            depth_gt_np = depth_gt_np.cpu().numpy()\n",
    "            \n",
    "            rgb_np_resized = cv2.resize(rgb_np, (640,192), interpolation=cv2.INTER_LINEAR)\n",
    "            depth_img_np_resized = cv2.resize(depth_img_np, (640,192), interpolation=cv2.INTER_LINEAR)\n",
    "            depth_gt_np_resized = cv2.resize(depth_gt_np, (640,192), interpolation=cv2.INTER_LINEAR)\n",
    "            \n",
    "            rgb_np_resized_color = rgb_np_resized\n",
    "            depth_img_np_resized_color = cv2.cvtColor(depth_img_np_resized, cv2.COLOR_GRAY2RGB)\n",
    "            depth_gt_np_resized_color = cv2.cvtColor(depth_gt_np_resized, cv2.COLOR_GRAY2RGB)    \n",
    "\n",
    "            input_rgb_pytorch = transforms.ToTensor()(rgb_np_resized_color)\n",
    "            input_lidar_pytorch = transforms.ToTensor()(depth_img_np_resized_color)\n",
    "            input_lidar_gt_pytorch = transforms.ToTensor()(depth_gt_np_resized_color)\n",
    "            \n",
    "            ########## network input pre-processing end ###############\n",
    "            \n",
    "            rgb_input.append(input_rgb_pytorch)\n",
    "            lidar_input.append(input_lidar_pytorch)\n",
    "            lidar_gt.append(input_lidar_gt_pytorch)\n",
    "            real_shape_input.append(real_shape)\n",
    "            shape_pad_input.append(shape_pad)\n",
    "            pc_rotated_input.append(pc_rotated)\n",
    "            RTs.append(RT)\n",
    "\n",
    "        if outlier_filter:\n",
    "            continue\n",
    "\n",
    "        rgb_input = torch.stack(rgb_input)\n",
    "        lidar_input = torch.stack(lidar_input)\n",
    "        rgb_resize = F.interpolate(rgb_input, size=[192, 640], mode=\"bilinear\")\n",
    "        lidar_resize = F.interpolate(lidar_input, size=[192, 640], mode=\"bilinear\")\n",
    "        \n",
    "#         print ('------lidar_input shape---------', lidar_input.shape)\n",
    "#         print ('------rgb_input shape---------', rgb_input.shape)\n",
    "\n",
    "        if _config['save_image']:\n",
    "            out0 = overlay_imgs(rgb_input[0], lidar_input)\n",
    "            print ('------out0 shape---------', np.array(out0).shape)\n",
    "            out0 = out0[:376, :1241, :]\n",
    "            cv2.imwrite(os.path.join(input_path, sample['rgb_name'][0]), out0[:, :, [2, 1, 0]]*255)\n",
    "            out1 = overlay_imgs(rgb_input[0], lidar_gt[0].unsqueeze(0))\n",
    "            out1 = out1[:376, :1241, :]\n",
    "            cv2.imwrite(os.path.join(gt_path, sample['rgb_name'][0]), out1[:, :, [2, 1, 0]]*255)\n",
    "\n",
    "            depth_img = depth_img.detach().cpu().numpy()\n",
    "            depth_img = (depth_img / np.max(depth_img)) * 255\n",
    "            cv2.imwrite(os.path.join(depth_path, sample['rgb_name'][0]), depth_img[0, :376, :1241])\n",
    "\n",
    "        if show:\n",
    "            out0 = overlay_imgs(rgb_input[0], lidar_input)\n",
    "            out1 = overlay_imgs(rgb_input[0], lidar_gt[0].unsqueeze(0))\n",
    "            cv2.imshow(\"INPUT\", out0[:, :, [2, 1, 0]])\n",
    "            cv2.imshow(\"GT\", out1[:, :, [2, 1, 0]])\n",
    "            cv2.waitKey(1)\n",
    "        \n",
    "        \"\"\" # Ver1.0 input tensor\n",
    "        lidar_input_nested = torch.cat([lidar_resize,lidar_resize,lidar_resize],1)\n",
    "        nested_input = two_images_side_by_side(rgb_resize, lidar_input_nested)\n",
    "        nested_input = torch.from_numpy(nested_input).cuda()\n",
    "        #nested_input = torch.from_numpy(nested_input).to(device)\n",
    "        nested_input = nested_input.permute(0,1,3,2)\n",
    "        nested_input_squeeze = torch.squeeze(nested_input)\n",
    "        nested_input_squeeze = nested_input_squeeze.permute(1,2,0)\n",
    "        sample_ndarray = nested_input_squeeze.cpu().numpy()\n",
    "        sample_ndarray = tvtf.normalize(tvtf.to_tensor(sample_ndarray), (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)).float()[None].cuda()\n",
    "\n",
    "\n",
    "        q_list = []\n",
    "        for i in range(256):\n",
    "            queries = []\n",
    "            for j in range(256 * 2):\n",
    "                queries.append([(j) / (256 * 2), i / 256])\n",
    "            queries = np.array(queries)\n",
    "            q_list.append(queries)\n",
    "        q_list = torch.tensor(q_list).float().cuda()\n",
    "        \"\"\"\n",
    "        \n",
    "        rgb = rgb_input.to(device)\n",
    "        lidar = lidar_input.to(device)\n",
    "        rgb_resize = rgb_resize.to(device)\n",
    "        lidar_resize = lidar_resize.to(device)\n",
    "        \n",
    "        target_transl = sample['tr_error'].cuda()\n",
    "        target_rot = sample['rot_error'].cuda()\n",
    "\n",
    "        # the initial calibration errors before sensor calibration\n",
    "        RT1 = RTs[0]\n",
    "        mis_calib = torch.stack(sample['initial_RT'])[1:]\n",
    "        mis_calib_list.append(mis_calib)\n",
    "\n",
    "        T_composed = RT1[:3, 3]\n",
    "        R_composed = quaternion_from_matrix(RT1)\n",
    "        errors_t[0].append(T_composed.norm().item())\n",
    "        errors_t2[0].append(T_composed)\n",
    "        errors_r[0].append(quaternion_distance(R_composed.unsqueeze(0),\n",
    "                                               torch.tensor([1., 0., 0., 0.], device=R_composed.device).unsqueeze(0),\n",
    "                                               R_composed.device))\n",
    "        # rpy_error = quaternion_to_tait_bryan(R_composed)\n",
    "        rpy_error = mat2xyzrpy(RT1)[3:]\n",
    "\n",
    "        rpy_error *= (180.0 / 3.141592)\n",
    "        errors_rpy[0].append(rpy_error)\n",
    "        log_string += [str(errors_t[0][-1]), str(errors_r[0][-1]), str(errors_t2[0][-1][0].item()),\n",
    "                       str(errors_t2[0][-1][1].item()), str(errors_t2[0][-1][2].item()),\n",
    "                       str(errors_rpy[0][-1][0].item()), str(errors_rpy[0][-1][1].item()),\n",
    "                       str(errors_rpy[0][-1][2].item())]\n",
    "\n",
    "        # if batch_idx == 0.:\n",
    "        #     print(f'Initial T_erorr: {errors_t[0]}')\n",
    "        #     print(f'Initial R_erorr: {errors_r[0]}')\n",
    "        start = 0\n",
    "        # t1 = time.time()\n",
    "\n",
    "        # Run model\n",
    "        with torch.no_grad():\n",
    "            for iteration in range(start, len(weights)):\n",
    "                # Run the i-th network\n",
    "                t1 = time.time()\n",
    "                if _config['iterative_method'] == 'single_range' or _config['iterative_method'] == 'single':\n",
    "                    T_predicted, R_predicted = models[0](rgb_resize, lidar_resize)\n",
    "                elif _config['iterative_method'] == 'multi_range':\n",
    "                    T_predicted, R_predicted = models[iteration](rgb_resize, lidar_resize)\n",
    "                run_time = time.time() - t1\n",
    "\n",
    "                if _config['rot_transl_separated'] and iteration == 0:\n",
    "                    T_predicted = torch.tensor([[0., 0., 0.]], device='cuda')\n",
    "                if _config['rot_transl_separated'] and iteration == 1:\n",
    "                    R_predicted = torch.tensor([[1., 0., 0., 0.]], device='cuda')\n",
    "\n",
    "                # Project the points in the new pose predicted by the i-th network\n",
    "                R_predicted = quat2mat(R_predicted[0])\n",
    "                T_predicted = tvector2mat(T_predicted[0])\n",
    "                RT_predicted = torch.mm(T_predicted, R_predicted)\n",
    "                RTs.append(torch.mm(RTs[iteration], RT_predicted)) # inv(H_gt)*H_pred_1*H_pred_2*.....H_pred_n\n",
    "                if iteration == 0:\n",
    "                    rotated_point_cloud = pc_rotated_input[0]\n",
    "                else:\n",
    "                    rotated_point_cloud = rotated_point_cloud\n",
    "\n",
    "                rotated_point_cloud = rotate_forward(rotated_point_cloud, RT_predicted) # H_pred*X_init\n",
    "\n",
    "                depth_img_pred, uv_pred, pc_pred_valid = lidar_project_depth(rotated_point_cloud, sample['calib'][0], real_shape_input[0]) # image_shape\n",
    "                depth_img_pred /= _config['max_depth']\n",
    "                depth_pred = F.pad(depth_img_pred, shape_pad_input[0])\n",
    "                lidar = depth_pred.unsqueeze(0)\n",
    "                lidar_resize = F.interpolate(lidar, size=[256, 512], mode=\"bilinear\")\n",
    "\n",
    "                if iteration == len(weights)-1 and _config['save_image']:\n",
    "                    # save the RGB pointcloud\n",
    "                    img = cv2.imread(sample['img_path'][0])\n",
    "                    R = img[uv_pred[:, 1], uv_pred[:, 0], 0] / 255\n",
    "                    G = img[uv_pred[:, 1], uv_pred[:, 0], 1] / 255\n",
    "                    B = img[uv_pred[:, 1], uv_pred[:, 0], 2] / 255\n",
    "                    pcl_pred = o3.geometry.PointCloud()\n",
    "                    pcl_pred.points = o3.utility.Vector3dVector(pc_pred_valid[:, :3])\n",
    "                    pcl_pred.colors = o3.utility.Vector3dVector(np.vstack((R, G, B)).T)\n",
    "\n",
    "                    # o3.draw_geometries(downpcd)\n",
    "                    o3.io.write_point_cloud(pc_pred_path + '/{}.pcd'.format(batch_idx), pcl_pred)\n",
    "\n",
    "\n",
    "                if _config['save_image']:\n",
    "                    out2 = overlay_imgs(rgb_input[0], lidar)\n",
    "                    out2 = out2[:376, :1241, :]\n",
    "                    cv2.imwrite(os.path.join(os.path.join(pred_path, 'iteration_'+str(iteration+1)),\n",
    "                                             sample['rgb_name'][0]), out2[:, :, [2, 1, 0]]*255)\n",
    "                if show:\n",
    "                    out2 = overlay_imgs(rgb_input[0], lidar)\n",
    "                    cv2.imshow(f'Pred_Iter_{iteration}', out2[:, :, [2, 1, 0]])\n",
    "                    cv2.waitKey(1)\n",
    "\n",
    "                # inv(H_init)*H_pred\n",
    "                T_composed = RTs[iteration + 1][:3, 3]\n",
    "                R_composed = quaternion_from_matrix(RTs[iteration + 1])\n",
    "                errors_t[iteration + 1].append(T_composed.norm().item())\n",
    "                errors_t2[iteration + 1].append(T_composed)\n",
    "                errors_r[iteration + 1].append(quaternion_distance(R_composed.unsqueeze(0),\n",
    "                                                                   torch.tensor([1., 0., 0., 0.], device=R_composed.device).unsqueeze(0),\n",
    "                                                                   R_composed.device))\n",
    "\n",
    "                # rpy_error = quaternion_to_tait_bryan(R_composed)\n",
    "                rpy_error = mat2xyzrpy(RTs[iteration + 1])[3:]\n",
    "                rpy_error *= (180.0 / 3.141592)\n",
    "                errors_rpy[iteration + 1].append(rpy_error)\n",
    "                log_string += [str(errors_t[iteration + 1][-1]), str(errors_r[iteration + 1][-1]),\n",
    "                               str(errors_t2[iteration + 1][-1][0].item()), str(errors_t2[iteration + 1][-1][1].item()),\n",
    "                               str(errors_t2[iteration + 1][-1][2].item()), str(errors_rpy[iteration + 1][-1][0].item()),\n",
    "                               str(errors_rpy[iteration + 1][-1][1].item()), str(errors_rpy[iteration + 1][-1][2].item())]\n",
    "\n",
    "        # run_time = time.time() - t1\n",
    "        total_time += run_time\n",
    "\n",
    "        # final calibration error\n",
    "        all_RTs.append(RTs[-1])\n",
    "        prev_RT = RTs[-1].inverse()\n",
    "        prev_tr_error = prev_RT[:3, 3].unsqueeze(0)\n",
    "        prev_rot_error = quaternion_from_matrix(prev_RT).unsqueeze(0)\n",
    "\n",
    "        if _config['save_log']:\n",
    "            log_file.writerow(log_string)\n",
    "\n",
    "    # Yaw（偏航）：欧拉角向量的y轴\n",
    "    # Pitch（俯仰）：欧拉角向量的x轴\n",
    "    # Roll（翻滚）： 欧拉角向量的z轴\n",
    "    # mis_calib_input[transl_x, transl_y, transl_z, rotx, roty, rotz] Nx6\n",
    "    mis_calib_input = torch.stack(mis_calib_list)[:, :, 0]\n",
    "\n",
    "    if _config['save_log']:\n",
    "        log_file.close()\n",
    "    print(\"Iterative refinement: \")\n",
    "    for i in range(len(weights) + 1):\n",
    "        errors_r[i] = torch.tensor(errors_r[i]).abs() * (180.0 / 3.141592)\n",
    "        errors_t[i] = torch.tensor(errors_t[i]).abs() * 100\n",
    "\n",
    "        for k in range(len(errors_rpy[i])):\n",
    "            # errors_rpy[i][k] = torch.tensor(errors_rpy[i][k])\n",
    "            # errors_t2[i][k] = torch.tensor(errors_t2[i][k]) * 100\n",
    "            errors_rpy[i][k] = errors_rpy[i][k].clone().detach().abs()\n",
    "            errors_t2[i][k] = errors_t2[i][k].clone().detach().abs() * 100\n",
    "\n",
    "        print(f\"Iteration {i}: \\tMean Translation Error: {errors_t[i].mean():.4f} cm \"\n",
    "              f\"     Mean Rotation Error: {errors_r[i].mean():.4f} °\")\n",
    "        print(f\"Iteration {i}: \\tMedian Translation Error: {errors_t[i].median():.4f} cm \"\n",
    "              f\"     Median Rotation Error: {errors_r[i].median():.4f} °\")\n",
    "        print(f\"Iteration {i}: \\tStd. Translation Error: {errors_t[i].std():.4f} cm \"\n",
    "              f\"     Std. Rotation Error: {errors_r[i].std():.4f} °\\n\")\n",
    "\n",
    "        # translation xyz\n",
    "        print(f\"Iteration {i}: \\tMean Translation X Error: {errors_t2[i][0].mean():.4f} cm \"\n",
    "              f\"     Median Translation X Error: {errors_t2[i][0].median():.4f} cm \"\n",
    "              f\"     Std. Translation X Error: {errors_t2[i][0].std():.4f} cm \")\n",
    "        print(f\"Iteration {i}: \\tMean Translation Y Error: {errors_t2[i][1].mean():.4f} cm \"\n",
    "              f\"     Median Translation Y Error: {errors_t2[i][1].median():.4f} cm \"\n",
    "              f\"     Std. Translation Y Error: {errors_t2[i][1].std():.4f} cm \")\n",
    "        print(f\"Iteration {i}: \\tMean Translation Z Error: {errors_t2[i][2].mean():.4f} cm \"\n",
    "              f\"     Median Translation Z Error: {errors_t2[i][2].median():.4f} cm \"\n",
    "              f\"     Std. Translation Z Error: {errors_t2[i][2].std():.4f} cm \\n\")\n",
    "\n",
    "        # rotation rpy[roll pitch yaw]\n",
    "        print(f\"Iteration {i}: \\tMean Rotation Roll Error: {errors_rpy[i][0].mean(): .4f} °\"\n",
    "              f\"     Median Rotation Roll Error: {errors_rpy[i][0].median():.4f} °\"\n",
    "              f\"     Std. Rotation Roll Error: {errors_rpy[i][0].std():.4f} °\")\n",
    "        print(f\"Iteration {i}: \\tMean Rotation Pitch Error: {errors_rpy[i][1].mean(): .4f} °\"\n",
    "              f\"     Median Rotation Pitch Error: {errors_rpy[i][1].median():.4f} °\"\n",
    "              f\"     Std. Rotation Pitch Error: {errors_rpy[i][1].std():.4f} °\")\n",
    "        print(f\"Iteration {i}: \\tMean Rotation Yaw Error: {errors_rpy[i][2].mean(): .4f} °\"\n",
    "              f\"     Median Rotation Yaw Error: {errors_rpy[i][2].median():.4f} °\"\n",
    "              f\"     Std. Rotation Yaw Error: {errors_rpy[i][2].std():.4f} °\\n\")\n",
    "\n",
    "\n",
    "        with open(os.path.join(_config['output'], 'results.txt'),\n",
    "                  'a', encoding='utf-8') as f:\n",
    "            f.write(f\"Iteration {i}: \\n\")\n",
    "            f.write(\"Translation Error && Rotation Error:\\n\")\n",
    "            f.write(f\"Iteration {i}: \\tMean Translation Error: {errors_t[i].mean():.4f} cm \"\n",
    "                    f\"     Mean Rotation Error: {errors_r[i].mean():.4f} °\\n\")\n",
    "            f.write(f\"Iteration {i}: \\tMedian Translation Error: {errors_t[i].median():.4f} cm \"\n",
    "                    f\"     Median Rotation Error: {errors_r[i].median():.4f} °\\n\")\n",
    "            f.write(f\"Iteration {i}: \\tStd. Translation Error: {errors_t[i].std():.4f} cm \"\n",
    "                    f\"     Std. Rotation Error: {errors_r[i].std():.4f} °\\n\\n\")\n",
    "\n",
    "            # translation xyz\n",
    "            f.write(\"Translation Error XYZ:\\n\")\n",
    "            f.write(f\"Iteration {i}: \\tMean Translation X Error: {errors_t2[i][0].mean():.4f} cm \"\n",
    "                    f\"     Median Translation X Error: {errors_t2[i][0].median():.4f} cm \"\n",
    "                    f\"     Std. Translation X Error: {errors_t2[i][0].std():.4f} cm \\n\")\n",
    "            f.write(f\"Iteration {i}: \\tMean Translation Y Error: {errors_t2[i][1].mean():.4f} cm \"\n",
    "                    f\"     Median Translation Y Error: {errors_t2[i][1].median():.4f} cm \"\n",
    "                    f\"     Std. Translation Y Error: {errors_t2[i][1].std():.4f} cm \\n\")\n",
    "            f.write(f\"Iteration {i}: \\tMean Translation Z Error: {errors_t2[i][2].mean():.4f} cm \"\n",
    "                    f\"     Median Translation Z Error: {errors_t2[i][2].median():.4f} cm \"\n",
    "                    f\"     Std. Translation Z Error: {errors_t2[i][2].std():.4f} cm \\n\\n\")\n",
    "\n",
    "            # rotation rpy[roll pitch yaw]\n",
    "            f.write(\"Rotation Error RPY:\\n\")\n",
    "            f.write(f\"Iteration {i}: \\tMean Rotation Roll Error: {errors_rpy[i][0].mean(): .4f} °\"\n",
    "                    f\"     Median Rotation Roll Error: {errors_rpy[i][0].median():.4f} °\"\n",
    "                    f\"     Std. Rotation Roll Error: {errors_rpy[i][0].std():.4f} °\\n\")\n",
    "            f.write(f\"Iteration {i}: \\tMean Rotation Pitch Error: {errors_rpy[i][1].mean(): .4f} °\"\n",
    "                    f\"     Median Rotation Pitch Error: {errors_rpy[i][1].median():.4f} °\"\n",
    "                    f\"     Std. Rotation Pitch Error: {errors_rpy[i][1].std():.4f} °\\n\")\n",
    "            f.write(f\"Iteration {i}: \\tMean Rotation Yaw Error: {errors_rpy[i][2].mean(): .4f} °\"\n",
    "                    f\"     Median Rotation Yaw Error: {errors_rpy[i][2].median():.4f} °\"\n",
    "                    f\"     Std. Rotation Yaw Error: {errors_rpy[i][2].std():.4f} °\\n\\n\\n\")\n",
    "\n",
    "    for i in range(len(errors_t2)):\n",
    "        errors_t2[i] = torch.stack(errors_t2[i]).abs() / 100\n",
    "        errors_rpy[i] = torch.stack(errors_rpy[i]).abs()\n",
    "\n",
    "    # mis_calib_input\n",
    "    # t_x = mis_calib_input[:, 0]\n",
    "    # t_y = mis_calib_input[:, 1]\n",
    "    # t_z = mis_calib_input[:, 2]\n",
    "    # r_roll = mis_calib_input[:, 5]\n",
    "    # r_pitch = mis_calib_input[:, 3]\n",
    "    # r_yaw = mis_calib_input[:, 4]\n",
    "\n",
    "    # plot_error\n",
    "    # plot_x = errors_t2[:, 0]\n",
    "    # plot_y = errors_t2[:, 1]\n",
    "    # plot_z = errors_t2[:, 2]\n",
    "    # plot_roll = errors_rpy[:, 0]\n",
    "    # plot_pitch = errors_rpy[:, 1]\n",
    "    # plot_yaw = errors_rpy[:, 2]\n",
    "\n",
    "    # translation error\n",
    "    # fig = plt.figure(figsize=(6, 3))  # 设置图大小 figsize=(6,3)\n",
    "    # plt.title('Calibration Translation Error')\n",
    "    plot_x = np.zeros((mis_calib_input.shape[0], 2))\n",
    "    plot_x[:, 0] = mis_calib_input[:, 0].cpu().numpy()\n",
    "    plot_x[:, 1] = errors_t2[-1][:, 0].cpu().numpy()\n",
    "    plot_x = plot_x[np.lexsort(plot_x[:, ::-1].T)]\n",
    "\n",
    "    plot_y = np.zeros((mis_calib_input.shape[0], 2))\n",
    "    plot_y[:, 0] = mis_calib_input[:, 1].cpu().numpy()\n",
    "    plot_y[:, 1] = errors_t2[-1][:, 1].cpu().numpy()\n",
    "    plot_y = plot_y[np.lexsort(plot_y[:, ::-1].T)]\n",
    "\n",
    "    plot_z = np.zeros((mis_calib_input.shape[0], 2))\n",
    "    plot_z[:, 0] = mis_calib_input[:, 2].cpu().numpy()\n",
    "    plot_z[:, 1] = errors_t2[-1][:, 2].cpu().numpy()\n",
    "    plot_z = plot_z[np.lexsort(plot_z[:, ::-1].T)]\n",
    "\n",
    "    N_interval = plot_x.shape[0] // N\n",
    "    plot_x = plot_x[::N_interval]\n",
    "    plot_y = plot_y[::N_interval]\n",
    "    plot_z = plot_z[::N_interval]\n",
    "\n",
    "    plt.plot(plot_x[:, 0], plot_x[:, 1], c='red', label='X')\n",
    "    plt.plot(plot_y[:, 0], plot_y[:, 1], c='blue', label='Y')\n",
    "    plt.plot(plot_z[:, 0], plot_z[:, 1], c='green', label='Z')\n",
    "    # plt.legend(loc='best')\n",
    "\n",
    "    if _config['out_fig_lg'] == 'EN':\n",
    "        plt.xlabel('Miscalibration (m)', font_EN)\n",
    "        plt.ylabel('Absolute Error (m)', font_EN)\n",
    "        plt.legend(loc='best', prop=font_EN)\n",
    "    elif _config['out_fig_lg'] == 'CN':\n",
    "        plt.xlabel('初始标定外参偏差/米', font_CN)\n",
    "        plt.ylabel('绝对误差/米', font_CN)\n",
    "        plt.legend(loc='best', prop=font_CN)\n",
    "\n",
    "    plt.xticks(fontproperties='Times New Roman', size=plt_size)\n",
    "    plt.yticks(fontproperties='Times New Roman', size=plt_size)\n",
    "\n",
    "    plt.savefig(os.path.join(results_path, 'xyz_plot.png'))\n",
    "    plt.close('all')\n",
    "\n",
    "    errors_t = errors_t[-1].numpy()\n",
    "    errors_t = np.sort(errors_t, axis=0)[:-10] # 去掉一些异常值\n",
    "    # plt.title('Calibration Translation Error Distribution')\n",
    "    plt.hist(errors_t / 100, bins=50)\n",
    "    # ax = plt.gca()\n",
    "    # ax.set_xlabel('Absolute Translation Error (m)')\n",
    "    # ax.set_ylabel('Number of instances')\n",
    "    # ax.set_xticks([0.00, 0.25, 0.00, 0.25, 0.50])\n",
    "\n",
    "    if _config['out_fig_lg'] == 'EN':\n",
    "        plt.xlabel('Absolute Translation Error (m)', font_EN)\n",
    "        plt.ylabel('Number of instances', font_EN)\n",
    "    elif _config['out_fig_lg'] == 'CN':\n",
    "        plt.xlabel('绝对平移误差/米', font_CN)\n",
    "        plt.ylabel('实验序列数目/个', font_CN)\n",
    "    plt.xticks(fontproperties='Times New Roman', size=plt_size)\n",
    "    plt.yticks(fontproperties='Times New Roman', size=plt_size)\n",
    "\n",
    "    plt.savefig(os.path.join(results_path, 'translation_error_distribution.png'))\n",
    "    plt.close('all')\n",
    "\n",
    "    # rotation error\n",
    "    # fig = plt.figure(figsize=(6, 3))  # 이미지 크기 설정 figsize=(6,3)\n",
    "    # plt.title('Calibration Rotation Error')\n",
    "    plot_pitch = np.zeros((mis_calib_input.shape[0], 2))\n",
    "    plot_pitch[:, 0] = mis_calib_input[:, 3].cpu().numpy() * (180.0 / 3.141592)\n",
    "    plot_pitch[:, 1] = errors_rpy[-1][:, 1].cpu().numpy()\n",
    "    plot_pitch = plot_pitch[np.lexsort(plot_pitch[:, ::-1].T)]\n",
    "\n",
    "    plot_yaw = np.zeros((mis_calib_input.shape[0], 2))\n",
    "    plot_yaw[:, 0] = mis_calib_input[:, 4].cpu().numpy() * (180.0 / 3.141592)\n",
    "    plot_yaw[:, 1] = errors_rpy[-1][:, 2].cpu().numpy()\n",
    "    plot_yaw = plot_yaw[np.lexsort(plot_yaw[:, ::-1].T)]\n",
    "\n",
    "    plot_roll = np.zeros((mis_calib_input.shape[0], 2))\n",
    "    plot_roll[:, 0] = mis_calib_input[:, 5].cpu().numpy() * (180.0 / 3.141592)\n",
    "    plot_roll[:, 1] = errors_rpy[-1][:, 0].cpu().numpy()\n",
    "    plot_roll = plot_roll[np.lexsort(plot_roll[:, ::-1].T)]\n",
    "\n",
    "    N_interval = plot_roll.shape[0] // N\n",
    "    plot_pitch = plot_pitch[::N_interval]\n",
    "    plot_yaw = plot_yaw[::N_interval]\n",
    "    plot_roll = plot_roll[::N_interval]\n",
    "\n",
    "    # Yaw : 오일러 각도 벡터의 y 축\n",
    "    # Pitch : 오일러 각도 벡터의 x 축\n",
    "    # Roll : 오일러 각도 벡터의 z 축\n",
    "\n",
    "    if _config['out_fig_lg'] == 'EN':\n",
    "        plt.plot(plot_yaw[:, 0], plot_yaw[:, 1], c='red', label='Yaw(Y)')\n",
    "        plt.plot(plot_pitch[:, 0], plot_pitch[:, 1], c='blue', label='Pitch(X)')\n",
    "        plt.plot(plot_roll[:, 0], plot_roll[:, 1], c='green', label='Roll(Z)')\n",
    "        plt.xlabel('Miscalibration (°)', font_EN)\n",
    "        plt.ylabel('Absolute Error (°)', font_EN)\n",
    "        plt.legend(loc='best', prop=font_EN)\n",
    "    elif _config['out_fig_lg'] == 'CN':\n",
    "        plt.plot(plot_yaw[:, 0], plot_yaw[:, 1], c='red', label='偏航角')\n",
    "        plt.plot(plot_pitch[:, 0], plot_pitch[:, 1], c='blue', label='俯仰角')\n",
    "        plt.plot(plot_roll[:, 0], plot_roll[:, 1], c='green', label='翻滚角')\n",
    "        plt.xlabel('初始标定外参偏差/度', font_CN)\n",
    "        plt.ylabel('绝对误差/度', font_CN)\n",
    "        plt.legend(loc='best', prop=font_CN)\n",
    "\n",
    "    plt.xticks(fontproperties='Times New Roman', size=plt_size)\n",
    "    plt.yticks(fontproperties='Times New Roman', size=plt_size)\n",
    "    plt.savefig(os.path.join(results_path, 'rpy_plot.png'))\n",
    "    plt.close('all')\n",
    "\n",
    "    errors_r = errors_r[-1].numpy()\n",
    "    errors_r = np.sort(errors_r, axis=0)[:-10] # 去掉一些异常值\n",
    "    # np.savetxt('rot_error.txt', arr_, fmt='%0.8f')\n",
    "    # print('max rotation_error: {}'.format(max(errors_r)))\n",
    "    # plt.title('Calibration Rotation Error Distribution')\n",
    "    plt.hist(errors_r, bins=50)\n",
    "    #plt.xlim([0, 1.5])  # x轴边界\n",
    "    #plt.xticks([0.0, 0.3, 0.6, 0.9, 1.2, 1.5])  # 设置x刻度\n",
    "    # ax = plt.gca()\n",
    "\n",
    "    if _config['out_fig_lg'] == 'EN':\n",
    "        plt.xlabel('Absolute Rotation Error (°)', font_EN)\n",
    "        plt.ylabel('Number of instances', font_EN)\n",
    "    elif _config['out_fig_lg'] == 'CN':\n",
    "        plt.xlabel('绝对旋转误差/度', font_CN)\n",
    "        plt.ylabel('实验序列数目/个', font_CN)\n",
    "    plt.xticks(fontproperties='Times New Roman', size=plt_size)\n",
    "    plt.yticks(fontproperties='Times New Roman', size=plt_size)\n",
    "    plt.savefig(os.path.join(results_path, 'rotation_error_distribution.png'))\n",
    "    plt.close('all')\n",
    "\n",
    "\n",
    "    if _config[\"save_name\"] is not None:\n",
    "        torch.save(torch.stack(errors_t).cpu().numpy(), f'./results_for_paper/{_config[\"save_name\"]}_errors_t')\n",
    "        torch.save(torch.stack(errors_r).cpu().numpy(), f'./results_for_paper/{_config[\"save_name\"]}_errors_r')\n",
    "        torch.save(torch.stack(errors_t2).cpu().numpy(), f'./results_for_paper/{_config[\"save_name\"]}_errors_t2')\n",
    "        torch.save(torch.stack(errors_rpy).cpu().numpy(), f'./results_for_paper/{_config[\"save_name\"]}_errors_rpy')\n",
    "\n",
    "    avg_time = total_time / len(TestImgLoader)\n",
    "    print(\"average runing time on {} iteration: {} s\".format(len(weights), avg_time))\n",
    "    print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
