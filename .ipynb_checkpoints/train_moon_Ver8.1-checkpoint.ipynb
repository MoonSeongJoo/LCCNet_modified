{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "strategic-devil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot use vispy, setting triangulate_corr as None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Copyright (C) 2020 Harbin Institute of Technology, China\n",
    "# Author: Xudong Lv (15B901019@hit.edu.cn)\n",
    "# Released under Creative Commons\n",
    "# Attribution-NonCommercial-ShareAlike 4.0 International License.\n",
    "# http://creativecommons.org/licenses/by-nc-sa/4.0/\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# import apex\n",
    "import mathutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as tvtf\n",
    "\n",
    "from sacred import Experiment\n",
    "from sacred.utils import apply_backspaces_and_linefeeds\n",
    "\n",
    "from DatasetLidarCamera_Ver8_1 import DatasetLidarCameraKittiOdometry\n",
    "from losses_Ver8_1 import DistancePoints3D, GeometricLoss, L1Loss, ProposedLoss, CombinedLoss\n",
    "\n",
    "\n",
    "from quaternion_distances import quaternion_distance\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from utils import (mat2xyzrpy, merge_inputs, overlay_imgs, quat2mat,\n",
    "                   quaternion_from_matrix, rotate_back, rotate_forward,\n",
    "                   tvector2mat)\n",
    "\n",
    "from LCCNet_COTR_moon_Ver8_1 import LCCNet\n",
    "from COTR.inference.sparse_engine_Ver3 import SparseEngine\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "ex = Experiment(\"LCCNet\" , interactive = True)\n",
    "ex.captured_out_filter = apply_backspaces_and_linefeeds\n",
    "from sacred import SETTINGS \n",
    "SETTINGS.CONFIG.READ_ONLY_CONFIG = False\n",
    "\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "# import multiprocessing as mp\n",
    "# mp.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "synthetic-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### dataset path root ############ \n",
    "\"\"\"\n",
    "root_dir = \"/mnt/data/kitti_odometry\"\n",
    "calib_path =\"data_odometry_calib\"\n",
    "image_path =\"data_odometry_color\"\n",
    "velodyne_path = \"data_odometry_velodyne\"\n",
    "imagegray_path = \"data_odometry_gray\"\n",
    "poses_path = \"data_odometry_poses\"\n",
    "\"\"\"\n",
    "#######################################\n",
    "# noinspection PyUnusedLocal\n",
    "@ex.config\n",
    "def config():\n",
    "    checkpoints = './checkpoints/'\n",
    "    dataset = 'kitti/odom' # 'kitti/raw'\n",
    "    data_folder = \"/mnt/data/kitti_odometry\"\n",
    "    use_reflectance = False\n",
    "    val_sequence = 23\n",
    "    epochs = 120\n",
    "    BASE_LEARNING_RATE = 5e-5 # 1e-4\n",
    "    loss = 'combined'\n",
    "    max_t = 0.2 # 1.5, 1.0,  0.5,  0.2,  0.1\n",
    "    max_r = 7.5 # 20.0, 10.0, 5.0,  2.0,  1.0\n",
    "    batch_size = 8  # 120\n",
    "    num_worker = 10\n",
    "    network = 'Res_f1'\n",
    "    optimizer = 'adam'\n",
    "    resume = True\n",
    "#     weights = '/root/work/LCCNet_Moon/checkpoints/kitti/odom/val_seq_21/models/checkpoint_r7.50_t0.20_e70_1.354.tar'\n",
    "    weights = None\n",
    "    rescale_rot = 100\n",
    "    rescale_transl = 1\n",
    "    precision = \"O0\"\n",
    "    norm = 'bn'\n",
    "    dropout = 0.0\n",
    "    max_depth = 80.\n",
    "    weight_point_cloud = 0.3\n",
    "    log_frequency = 1000\n",
    "    print_frequency = 50\n",
    "    starting_epoch = 0\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1, 2, 3'\n",
    "\n",
    "EPOCH = 1\n",
    "def _init_fn(worker_id, seed):\n",
    "    seed = seed + worker_id + EPOCH*100\n",
    "    print(f\"Init worker {worker_id} with seed {seed}\")\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def get_2D_lidar_projection(pcl, cam_intrinsic):\n",
    "    pcl_xyz = cam_intrinsic @ pcl.T\n",
    "    pcl_xyz = pcl_xyz.T\n",
    "    pcl_z = pcl_xyz[:, 2]\n",
    "    pcl_xyz = pcl_xyz / (pcl_xyz[:, 2, None] + 1e-10)\n",
    "    pcl_uv = pcl_xyz[:, :2]\n",
    "\n",
    "    return pcl_uv, pcl_z\n",
    "\n",
    "\n",
    "def lidar_project_depth(pc_rotated, cam_calib, img_shape):\n",
    "    pc_rotated = pc_rotated[:3, :].detach().cpu().numpy()\n",
    "    cam_intrinsic = cam_calib.numpy()\n",
    "    pcl_uv, pcl_z = get_2D_lidar_projection(pc_rotated.T, cam_intrinsic)\n",
    "    mask = (pcl_uv[:, 0] > 0) & (pcl_uv[:, 0] < img_shape[1]) & (pcl_uv[:, 1] > 0) & (\n",
    "            pcl_uv[:, 1] < img_shape[0]) & (pcl_z > 0)\n",
    "    pcl_uv = pcl_uv[mask]\n",
    "    pcl_z = pcl_z[mask]\n",
    "    pcl_uv = pcl_uv.astype(np.uint32)\n",
    "    pcl_z = pcl_z.reshape(-1, 1)\n",
    "    depth_img = np.zeros((img_shape[0], img_shape[1], 1))\n",
    "    depth_img[pcl_uv[:, 1], pcl_uv[:, 0]] = pcl_z\n",
    "    depth_img = torch.from_numpy(depth_img.astype(np.float32))\n",
    "    depth_img = depth_img.cuda()\n",
    "    depth_img = depth_img.permute(2, 0, 1)\n",
    "\n",
    "    return depth_img, pcl_uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comparable-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN training\n",
    "@ex.capture\n",
    "def train(model, optimizer, rgb_input, dense_depth_input, corrs , target_transl, target_rot, loss_fn, point_clouds, loss):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    \n",
    "    queries     = corrs[:, :, :2]\n",
    "    corr_target = corrs[:, :, 2:]\n",
    "\n",
    "    # Run model\n",
    "    #transl_err, rot_err = model(rgb_img, refl_img)\n",
    "    transl_err, rot_err , corr_pred , cycle , mask = model(rgb_input, dense_depth_input, queries , corr_target)\n",
    "    \"\"\"\n",
    "    print(\"transl_err==========\",transl_err)\n",
    "    print(\"rot_err=============\",rot_err)\n",
    "    print(\"point_clouds=============\",point_clouds)\n",
    "    print(\"target_transl=============\",target_transl)\n",
    "    print(\"target_rot=============\",target_rot)\n",
    "    \"\"\"\n",
    "    \n",
    "    if loss == 'points_distance' or loss == 'combined':\n",
    "        losses = loss_fn(point_clouds, target_transl, target_rot, transl_err, rot_err , corr_target , corr_pred , queries ,cycle, mask)\n",
    "    else:\n",
    "        losses = loss_fn(target_transl, target_rot, transl_err, rot_err)\n",
    "    \n",
    "    #print(\"losses=============\",losses)\n",
    "    if loss == 'points_distance' or loss == 'combined':\n",
    "        losses['total_loss'].backward()\n",
    "    else: \n",
    "        losses.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    return losses, rot_err, transl_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "established-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN test\n",
    "@ex.capture\n",
    "def val(model, rgb_input, dense_depth_input ,corrs ,target_transl, target_rot, loss_fn, point_clouds, loss):\n",
    "    model.eval()\n",
    "\n",
    "    queries     = corrs[:, :, :2]\n",
    "    corr_target = corrs[:, :, 2:]\n",
    "\n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        #transl_err, rot_err = model(rgb_img, refl_img)\n",
    "        transl_err, rot_err,corr_pred ,cycle , mask = model(rgb_input,dense_depth_input, queries ,corr_target)\n",
    "\n",
    "    if loss == 'points_distance' or loss == 'combined':\n",
    "        losses = loss_fn(point_clouds, target_transl, target_rot, transl_err, rot_err, corr_target , corr_pred ,queries ,cycle, mask)\n",
    "    else:\n",
    "        losses = loss_fn(target_transl, target_rot, transl_err, rot_err)\n",
    "\n",
    "    # if loss != 'points_distance':\n",
    "    #     total_loss = loss_fn(target_transl, target_rot, transl_err, rot_err)\n",
    "    # else:\n",
    "    #     total_loss = loss_fn(point_clouds, target_transl, target_rot, transl_err, rot_err)\n",
    "\n",
    "    total_trasl_error = torch.tensor(0.0).cuda()\n",
    "    target_transl = torch.tensor(target_transl).cuda()\n",
    "    total_rot_error = quaternion_distance(target_rot, rot_err, target_rot.device)\n",
    "    total_rot_error = total_rot_error * 180. / math.pi\n",
    "    for j in range(rgb_input.shape[0]):\n",
    "        total_trasl_error += torch.norm(target_transl[j] - transl_err[j]) * 100.\n",
    "\n",
    "    # # output image: The overlay image of the input rgb image and the projected lidar pointcloud depth image\n",
    "    # cam_intrinsic = camera_model[0]\n",
    "    # rotated_point_cloud =\n",
    "    # R_predicted = quat2mat(R_predicted[0])\n",
    "    # T_predicted = tvector2mat(T_predicted[0])\n",
    "    # RT_predicted = torch.mm(T_predicted, R_predicted)\n",
    "    # rotated_point_cloud = rotate_forward(rotated_point_cloud, RT_predicted)\n",
    "\n",
    "    return losses, total_trasl_error.item(), total_rot_error.sum().item(), rot_err, transl_err\n",
    "#     return losses, total_trasl_error.item(), rot_err, transl_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "invisible-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ex.automain\n",
    "@ex.main\n",
    "def main(_config, _run, seed):\n",
    "    global EPOCH\n",
    "    print('Loss Function Choice: {}'.format(_config['loss']))\n",
    "\n",
    "    if _config['val_sequence'] is None:\n",
    "        raise TypeError('val_sequences cannot be None')\n",
    "    else:\n",
    "        _config['val_sequence'] = f\"{_config['val_sequence']:02d}\"\n",
    "        print(\"Val Sequence: \", _config['val_sequence'])\n",
    "        dataset_class = DatasetLidarCameraKittiOdometry\n",
    "        \n",
    "    img_shape = (384, 1280) # 네트워크의 입력 규모\n",
    "    input_size = (256, 512)\n",
    "    _config[\"checkpoints\"] = os.path.join(_config[\"checkpoints\"], _config['dataset'])\n",
    "\n",
    "    dataset_train = dataset_class(_config['data_folder'], max_r=_config['max_r'], max_t=_config['max_t'],\n",
    "                                  split='train', use_reflectance=_config['use_reflectance'],\n",
    "                                  val_sequence=_config['val_sequence'])\n",
    "    dataset_val = dataset_class(_config['data_folder'], max_r=_config['max_r'], max_t=_config['max_t'],\n",
    "                                split='val', use_reflectance=_config['use_reflectance'],\n",
    "                                val_sequence=_config['val_sequence'])\n",
    "        \n",
    "    train_dataset_size = len(dataset_train)\n",
    "    val_dataset_size = len(dataset_val)\n",
    "    print('Number of the train dataset: {}'.format(train_dataset_size))\n",
    "    print('Number of the val dataset: {}'.format(val_dataset_size))\n",
    "    \n",
    "#     ##### verify - monitoring dataset train ##########\n",
    "#     rgb = dataset_train[129]['rgb']\n",
    "#     depth_img =  dataset_train[129]['depth_img']\n",
    "#     depth_gt_img = dataset_train[129]['depth_gt']\n",
    "\n",
    "\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.subplot(311)\n",
    "#     plt.imshow(rgb)\n",
    "#     plt.title(\"After RGB_resize\", fontsize=22)\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     plt.subplot(312)\n",
    "#     plt.imshow(depth_img, cmap='magma')\n",
    "#     plt.title(\"After miscalibrated_Lidar_resize\", fontsize=22)\n",
    "#     plt.axis('off');  \n",
    "    \n",
    "#     plt.subplot(313)\n",
    "#     plt.imshow(depth_gt_img, cmap='magma')\n",
    "#     plt.title(\"After Lidar_gt_resize\", fontsize=22)\n",
    "#     plt.axis('off'); \n",
    "    \n",
    "    ####### searching pre-trained model parameter dir ##############\n",
    "    model_savepath = os.path.join(_config['checkpoints'], 'val_seq_' + _config['val_sequence'], 'models')\n",
    "    if not os.path.exists(model_savepath):\n",
    "        os.makedirs(model_savepath)\n",
    "    log_savepath = os.path.join(_config['checkpoints'], 'val_seq_' + _config['val_sequence'], 'log')\n",
    "    print ('log_savepath :' , log_savepath)\n",
    "    if not os.path.exists(log_savepath):\n",
    "        os.makedirs(log_savepath)\n",
    "    \n",
    "    train_writer = SummaryWriter(os.path.join(log_savepath, 'train'))\n",
    "    val_writer = SummaryWriter(os.path.join(log_savepath, 'val'))\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "\n",
    "    def init_fn(x): return _init_fn(x, seed)\n",
    "    \n",
    "    # Training and validation set creation\n",
    "    num_worker = _config['num_worker']\n",
    "    batch_size = _config['batch_size']\n",
    "    print(\"batch_size=\" , batch_size)\n",
    "    TrainImgLoader = torch.utils.data.DataLoader(dataset=dataset_train,\n",
    "                                                 shuffle=True,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 num_workers=num_worker,\n",
    "                                                 worker_init_fn=init_fn,\n",
    "                                                 collate_fn=merge_inputs,\n",
    "                                                 drop_last=False,\n",
    "                                                 pin_memory=True)\n",
    "\n",
    "    ValImgLoader = torch.utils.data.DataLoader(dataset=dataset_val,\n",
    "                                                shuffle=False,\n",
    "                                                batch_size=batch_size,\n",
    "                                                num_workers=num_worker,\n",
    "                                                worker_init_fn=init_fn,\n",
    "                                                collate_fn=merge_inputs,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True)\n",
    "\n",
    "    print(len(TrainImgLoader))\n",
    "    print(len(ValImgLoader))\n",
    "    \n",
    "    # loss function choice\n",
    "    if _config['loss'] == 'simple':\n",
    "        loss_fn = ProposedLoss(_config['rescale_transl'], _config['rescale_rot'])\n",
    "    elif _config['loss'] == 'geometric':\n",
    "        loss_fn = GeometricLoss()\n",
    "        loss_fn = loss_fn.cuda()\n",
    "    elif _config['loss'] == 'points_distance':\n",
    "        loss_fn = DistancePoints3D()\n",
    "    elif _config['loss'] == 'L1':\n",
    "        loss_fn = L1Loss(_config['rescale_transl'], _config['rescale_rot'])\n",
    "    elif _config['loss'] == 'combined':\n",
    "        loss_fn = CombinedLoss(_config['rescale_transl'], _config['rescale_rot'], _config['weight_point_cloud'])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown Loss Function\")\n",
    "    \n",
    "    # network choice and settings\n",
    "    if _config['network'].startswith('Res'):\n",
    "        feat = 1\n",
    "        md = 4\n",
    "        split = _config['network'].split('_')\n",
    "        for item in split[1:]:\n",
    "            if item.startswith('f'):\n",
    "                feat = int(item[-1])\n",
    "            elif item.startswith('md'):\n",
    "                md = int(item[2:])\n",
    "        assert 0 < feat < 7, \"Feature Number from PWC have to be between 1 and 6\"\n",
    "        assert 0 < md, \"md must be positive\"   \n",
    "        ### netwrok define ####\n",
    "        model = LCCNet(input_size, use_feat_from=feat, md=md,\n",
    "                         use_reflectance=_config['use_reflectance'], dropout=_config['dropout'],\n",
    "                         Action_Func='leakyrelu', attention=False, res_num=18)\n",
    "    else:\n",
    "        raise TypeError(\"Network unknown\")\n",
    "    if _config['weights'] is not None:\n",
    "        print(f\"Loading weights from {_config['weights']}\")\n",
    "        checkpoint = torch.load(_config['weights'], map_location='cpu')\n",
    "        print('-------checkpoint-keys-------',checkpoint.keys() )\n",
    "        saved_state_dict = checkpoint['state_dict']\n",
    "        model.load_state_dict(saved_state_dict)\n",
    "\n",
    "    ########### parallel gpu loding ###########\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.cuda()\n",
    "\n",
    "    print('Number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n",
    "\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    \n",
    "    ############ optimizer setting ##############\n",
    "    if _config['loss'] == 'geometric':\n",
    "        parameters += list(loss_fn.parameters())\n",
    "    if _config['optimizer'] == 'adam':\n",
    "        optimizer = optim.Adam(parameters, lr=_config['BASE_LEARNING_RATE'], weight_decay=5e-6)\n",
    "        # Probably this scheduler is not used\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 50, 70], gamma=0.5)\n",
    "    else:\n",
    "        optimizer = optim.SGD(parameters, lr=_config['BASE_LEARNING_RATE'], momentum=0.9,\n",
    "                              weight_decay=5e-6, nesterov=True)\n",
    "    \n",
    "    ########## training resume setting ###############\n",
    "    starting_epoch = _config['starting_epoch']\n",
    "    if _config['weights'] is not None and _config['resume']:\n",
    "        checkpoint = torch.load(_config['weights'], map_location='cpu')\n",
    "        opt_state_dict = checkpoint['optimizer']\n",
    "        optimizer.load_state_dict(opt_state_dict)\n",
    "        if starting_epoch != 0:\n",
    "            starting_epoch = checkpoint['epoch']\n",
    "\n",
    "    ######### training epoch ########################3\n",
    "    start_full_time = time.time()\n",
    "    BEST_VAL_LOSS = 10000.\n",
    "    old_save_filename = None\n",
    "\n",
    "    train_iter = 0\n",
    "    val_iter = 0\n",
    "    real_shape = [376 , 1241 ,3]\n",
    "    \n",
    "    for epoch in range(starting_epoch, _config['epochs'] + 1):\n",
    "        EPOCH = epoch\n",
    "        print('This is %d-th epoch' % epoch)\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        total_train_loss = 0\n",
    "        local_loss = 0.\n",
    "        total_val_loss = 0.\n",
    "        total_val_t = 0.\n",
    "        total_val_r = 0.\n",
    "        \n",
    "        if _config['optimizer'] != 'adam':\n",
    "            _run.log_scalar(\"LR\", _config['BASE_LEARNING_RATE'] *\n",
    "                            math.exp((1 - epoch) * 4e-2), epoch)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = _config['BASE_LEARNING_RATE'] * \\\n",
    "                                    math.exp((1 - epoch) * 4e-2)\n",
    "        else:\n",
    "            #scheduler.step(epoch%100)\n",
    "            _run.log_scalar(\"LR\", scheduler.get_lr()[0])\n",
    "    \n",
    "    ########### traing batch #############################\n",
    "        time_for_50ep = time.time()\n",
    " \n",
    "        for batch_idx, sample in enumerate(TrainImgLoader):\n",
    "            print(f'batch {batch_idx+1}/{len(TrainImgLoader)}', end='\\r')\n",
    "            start_time = time.time()\n",
    "#             lidar_input = []\n",
    "            rgb_input = []\n",
    "            lidar_gt_input = []\n",
    "#             query_input = []\n",
    "#             query_gt_input = []\n",
    "            corrs_input =[]\n",
    "            dense_depth_img_input =[]\n",
    "            pc_rotated_input = []\n",
    "\n",
    "            # gt pose\n",
    "            sample['tr_error'] = sample['tr_error'].cuda()\n",
    "            sample['rot_error'] = sample['rot_error'].cuda()\n",
    "            \n",
    "            start_preprocess = time.time()\n",
    "            for idx in range(len(sample['rgb'])):\n",
    "                rgb = sample['rgb'][idx].cuda()\n",
    "#                 depth_img = sample['depth_img'][idx].cuda()\n",
    "                lidar_gt = sample['lidar_gt'][idx].cuda()\n",
    "#                 query = sample['gt_uv'][idx].cuda()\n",
    "#                 query_gt = sample['uv'][idx].cuda()\n",
    "                corrs = sample['corrs'][idx].cuda()\n",
    "                dense_depth_img = sample['dense_depth_img'][idx].cuda()\n",
    "                pc_rotated = sample['pc_rotated'][idx].cuda()\n",
    "                \n",
    "        \n",
    "                # batch stack \n",
    "                rgb_input.append(rgb)\n",
    "#                 lidar_input.append(depth_img)\n",
    "                lidar_gt_input.append(lidar_gt)\n",
    "#                 query_input.append(query)\n",
    "#                 query_gt_input.append(query_gt)\n",
    "                corrs_input.append(corrs)\n",
    "                dense_depth_img_input.append(dense_depth_img)\n",
    "                pc_rotated_input.append(pc_rotated)\n",
    "            \n",
    "            rgb_input = torch.stack(rgb_input)\n",
    "#             lidar_input = torch.stack(lidar_input)\n",
    "#             query_input = torch.stack(query_input)\n",
    "#             query_gt_input = torch.stack(query_gt_input)\n",
    "            corrs_input = torch.stack(corrs_input)\n",
    "            dense_depth_img_input = torch.stack(dense_depth_img_input)\n",
    "            \n",
    "            rgb_input = rgb_input.permute(0,2,3,1)\n",
    "#             sbs_img = rgb_input.clone()\n",
    "#             rgb_show = rgb_show.permute(0,2,3,1)\n",
    "#             rgb_show = sbs_img[: , : , :, :640]\n",
    "#             lidar_show = sbs_img[: , : , :, 640:]\n",
    "#             print ('rgb_show shape' , rgb_show.shape)\n",
    "#             lidar_input = lidar_input.permute(0,3,1,2)\n",
    "            dense_depth_img_input = dense_depth_img_input.permute(0,2,3,1)\n",
    "            \n",
    "            end_preprocess = time.time()\n",
    "#             print(\"rgb_input===========\",rgb_input.shape)\n",
    "#             print(\"lidar_input===========\",lidar_input.shape)\n",
    "#             print(\"query_input===========\",query_input.shape)\n",
    "#             print(\"query_gt_input===========\",query_gt_input.shape)\n",
    "#             print(\"corrs_input===========\",corrs_input.shape)\n",
    "#             print(\"dense_depth_img_input===========\",dense_depth_img_input.shape)\n",
    "            \n",
    "            loss, R_predicted,  T_predicted = train(model, optimizer, rgb_input, dense_depth_img_input , corrs_input ,\n",
    "                                      sample['tr_error'], sample['rot_error'],\n",
    "                                      loss_fn, sample['point_cloud'], _config['loss'])\n",
    "        \n",
    "            if _config['loss'] == 'points_distance' or _config['loss'] == 'combined':\n",
    "                local_loss += loss['total_loss'].item()\n",
    "            else :\n",
    "                local_loss += loss.item()\n",
    "            \n",
    "            train_local_loss = local_loss/50\n",
    "\n",
    "            if batch_idx % _config['log_frequency'] == 0:\n",
    "                show_idx = 0\n",
    "                # output image: The overlay image of the input rgb image\n",
    "                # and the projected lidar pointcloud depth image\n",
    "                rotated_point_cloud = pc_rotated_input[show_idx]\n",
    "                R_predicted = quat2mat(R_predicted[show_idx])\n",
    "                T_predicted = tvector2mat(T_predicted[show_idx])\n",
    "                RT_predicted = torch.mm(T_predicted, R_predicted)\n",
    "                rotated_point_cloud = rotate_forward(rotated_point_cloud, RT_predicted)\n",
    "\n",
    "#                 depth_pred, uv = lidar_project_depth(rotated_point_cloud,\n",
    "#                                                     sample['calib'][show_idx],\n",
    "#                                                     real_shape_input[show_idx]) # or image_shape\n",
    "                depth_pred, uv = lidar_project_depth(rotated_point_cloud,\n",
    "                                                    sample['calib'][show_idx],\n",
    "                                                    real_shape) # or image_shape\n",
    "                depth_pred /= _config['max_depth']\n",
    "#                 depth_pred = F.pad(depth_pred, shape_pad_input[show_idx])\n",
    "#                 print ('depth_pred shape' , depth_pred.shape)\n",
    "#                 print ('rgb_show shape' , rgb_show[show_idx].shape)\n",
    "#                 print ('lidar_show shape' , lidar_show[show_idx].shape)\n",
    "#                 print ('lidar_gt show shape' , lidar_gt_input[show_idx].shape)\n",
    "                depth_pred = depth_pred.permute(1,2,0)\n",
    "                depth_pred_np = depth_pred.cpu().numpy()\n",
    "                depth_pred_np_resized = cv2.resize(depth_pred_np, (640,192), interpolation=cv2.INTER_LINEAR)\n",
    "                depth_pred_np_resized_tensor = transforms.ToTensor()(depth_pred_np_resized)\n",
    "#                 depth_pred_np_resized_tensor = depth_pred_np_resized_tensor.permute(1,2,0)\n",
    "#                 print ('depth_pred_np_resized_tensor shape' , depth_pred_np_resized_tensor.shape)\n",
    "\n",
    "#                 pred_show = overlay_imgs(rgb_show[show_idx], depth_pred_np_resized_tensor.unsqueeze(0))\n",
    "#                 input_show = overlay_imgs(rgb_show[show_idx], lidar_show[show_idx].unsqueeze(0))\n",
    "#                 gt_show = overlay_imgs(rgb_show[show_idx], lidar_gt_input[show_idx].unsqueeze(0))\n",
    "                \n",
    "#                 print ('pred_show type' , type(pred_show))\n",
    "\n",
    "#                 pred_show = torch.from_numpy(np.asarray(pred_show))\n",
    "#                 pred_show = pred_show.permute(2, 0, 1)\n",
    "#                 input_show = torch.from_numpy(input_show)\n",
    "#                 input_show = input_show.permute(2, 0, 1)\n",
    "#                 gt_show = torch.from_numpy(gt_show)\n",
    "#                 gt_show = gt_show.permute(2, 0, 1)\n",
    "\n",
    "#                 train_writer.add_image(\"rgb_show\", rgb_show[show_idx], train_iter)\n",
    "                train_writer.add_image(\"gt_lidar\", lidar_gt_input[show_idx], train_iter)\n",
    "#                 train_writer.add_image(\"miscalibrated_lidar\", lidar_show[show_idx], train_iter)\n",
    "                train_writer.add_image(\"pred_lidar\", depth_pred_np_resized_tensor, train_iter)\n",
    "\n",
    "                if _config['loss'] == 'combined':\n",
    "                    train_writer.add_scalar(\"Loss_Point_clouds\", loss['point_clouds_loss'].item(), train_iter)\n",
    "                    train_writer.add_scalar(\"correspondence_matching\", loss['corr_loss'].item(), train_iter)\n",
    "                    train_writer.add_scalar(\"Loss_Total\", loss['total_loss'].item(), train_iter)\n",
    "                    train_writer.add_scalar(\"Loss_Translation\", loss['transl_loss'].item(), train_iter)\n",
    "                    train_writer.add_scalar(\"Loss_Rotation\", loss['rot_loss'].item(), train_iter)\n",
    "                else : \n",
    "                    train_writer.add_scalar(\"Loss_Total\", loss.item(), train_iter)\n",
    "            \n",
    "            if batch_idx % 50 == 0 and batch_idx != 0:\n",
    "\n",
    "                print(f'Iter {batch_idx}/{len(TrainImgLoader)} training loss = {local_loss/50:.3f}, '\n",
    "                      f'time = {(time.time() - start_time)/rgb_input.shape[0]:.4f}, '\n",
    "                      #f'time_preprocess = {(end_preprocess-start_preprocess)/lidar_input.shape[0]:.4f}, '\n",
    "                      f'time for 50 iter: {time.time()-time_for_50ep:.4f}')\n",
    "                time_for_50ep = time.time()\n",
    "                _run.log_scalar(\"Loss\", local_loss/50, train_iter)\n",
    "                local_loss = 0.\n",
    "                \n",
    "                #train_loss = train_local_loss / len(dataset_train)\n",
    "                ######### save network model for intermediate verification #####################  \n",
    "                if train_local_loss < 0.1:\n",
    "                    #if val_loss < BEST_VAL_LOSS:\n",
    "                #    BEST_VAL_LOSS = val_loss\n",
    "                    #_run.result = BEST_VAL_LOSS\n",
    "                    if _config['rescale_transl'] > 0:\n",
    "                        _run.result = total_val_t / len(dataset_val)\n",
    "                    else:\n",
    "                        _run.result = total_val_t / len(dataset_val)\n",
    "                        #_run.result = total_val_r / len(dataset_val)\n",
    "                    savefilename = f'{model_savepath}/checkpoint_r{_config[\"max_r\"]:.2f}_t{_config[\"max_t\"]:.2f}_e{epoch}_{train_local_loss:.3f}.tar'\n",
    "                    torch.save({\n",
    "                        'config': _config,\n",
    "                        'epoch': epoch,\n",
    "                        # 'state_dict': model.state_dict(), # single gpu\n",
    "                        'state_dict': model.module.state_dict(), # multi gpu\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'train_loss': total_train_loss / len(dataset_train),\n",
    "                        'val_loss': total_val_loss / len(dataset_val),\n",
    "                    }, savefilename)\n",
    "                    print(f'Model saved as {savefilename}')\n",
    "                    if old_save_filename is not None:\n",
    "                        if os.path.exists(old_save_filename):\n",
    "                            os.remove(old_save_filename)\n",
    "                    old_save_filename = savefilename                \n",
    "            \n",
    "            if _config['loss'] == 'points_distance' or _config['loss'] == 'combined':\n",
    "                total_train_loss += loss['total_loss'].item() * len(sample['rgb'])\n",
    "            else : \n",
    "                total_train_loss += loss.item() * len(sample['rgb'])\n",
    "            train_iter += 1\n",
    "            # total_iter += len(sample['rgb'])\n",
    "\n",
    "        print(\"------------------------------------\")\n",
    "        print('epoch %d total training loss = %.3f' % (epoch, total_train_loss / len(dataset_train)))\n",
    "        print('Total epoch time = %.2f' % (time.time() - epoch_start_time))\n",
    "        print(\"------------------------------------\")\n",
    "        _run.log_scalar(\"Total training loss\", total_train_loss / len(dataset_train), epoch)\n",
    "\n",
    "       ## Validation ##\n",
    "        total_val_loss = 0.\n",
    "        total_val_t = 0.\n",
    "        total_val_r = 0.\n",
    "\n",
    "        local_loss = 0.0\n",
    "\n",
    "        for batch_idx, sample in enumerate(ValImgLoader):\n",
    "            print(f'batch {batch_idx+1}/{len(ValImgLoader)}', end='\\r')\n",
    "            start_time = time.time()\n",
    "            lidar_input = []\n",
    "            rgb_input = []\n",
    "#             lidar_gt_input = []\n",
    "#             query_input = []\n",
    "#             query_gt_input = []\n",
    "            corrs_input =[]\n",
    "            dense_depth_img_input =[]\n",
    "\n",
    "            # gt pose\n",
    "            sample['tr_error'] = sample['tr_error'].cuda()\n",
    "            sample['rot_error'] = sample['rot_error'].cuda()\n",
    "\n",
    "            for idx in range(len(sample['rgb'])):\n",
    "                rgb = sample['rgb'][idx].cuda()\n",
    "#                 depth_img = sample['depth_img'][idx].cuda()\n",
    "#                 depth_img_gt = sample['depth_gt'][idx].cuda()\n",
    "#                 query = sample['gt_uv'][idx].cuda()\n",
    "#                 query_gt = sample['uv'][idx].cuda()\n",
    "                corrs = sample['corrs'][idx].cuda()\n",
    "                dense_depth_img = sample['dense_depth_img'][idx].cuda()                \n",
    "                \n",
    "                # batch stack \n",
    "                rgb_input.append(rgb)\n",
    "#                 lidar_input.append(depth_img)\n",
    "#                 lidar_gt_input.append(depth_img_gt)\n",
    "#                 query_input.append(query)\n",
    "#                 query_gt_input.append(query_gt)\n",
    "                corrs_input.append(corrs)\n",
    "                dense_depth_img_input.append(dense_depth_img)            \n",
    "            \n",
    "            rgb_input = torch.stack(rgb_input)\n",
    "#             lidar_input = torch.stack(lidar_input)\n",
    "#             query_input = torch.stack(query_input)\n",
    "#             query_gt_input = torch.stack(query_gt_input)\n",
    "            corrs_input = torch.stack(corrs_input)\n",
    "            dense_depth_img_input = torch.stack(dense_depth_img_input)            \n",
    "            \n",
    "            rgb_input = rgb_input.permute(0,2,3,1)\n",
    "#             lidar_input = lidar_input.permute(0,3,1,2)\n",
    "            dense_depth_img_input = dense_depth_img_input.permute(0,2,3,1)\n",
    "            \n",
    "            loss, trasl_e, rot_e, R_predicted,  T_predicted = val(model, rgb_input, dense_depth_img_input, corrs_input ,\n",
    "                                                                  sample['tr_error'], sample['rot_error'],\n",
    "                                                                  loss_fn, sample['point_cloud'], _config['loss'])\n",
    "\n",
    "            total_val_t += trasl_e\n",
    "            total_val_r += rot_e\n",
    "            if _config['loss'] == 'points_distance' or _config['loss'] == 'combined':\n",
    "                local_loss += loss['total_loss'].item()\n",
    "            else : \n",
    "                local_loss += loss.item()\n",
    "\n",
    "            if batch_idx % _config['log_frequency'] == 0:\n",
    "                if _config['loss'] == 'combined':\n",
    "                    val_writer.add_scalar(\"Loss_Translation\", trasl_e, val_iter)\n",
    "                    val_writer.add_scalar(\"Loss_Rotation\", rot_e, val_iter)\n",
    "                    val_writer.add_scalar(\"Total_Loss\", loss['total_loss'].item(), val_iter)\n",
    "                else : \n",
    "                    val_writer.add_scalar(\"Loss_Translation\", trasl_e, val_iter)\n",
    "                    val_writer.add_scalar(\"Loss_Rotation\", rot_e, val_iter)\n",
    "                    val_writer.add_scalar(\"Total_Loss\", loss['total_loss'].item(), val_iter)\n",
    "            \n",
    "            if batch_idx % 50 == 0 and batch_idx != 0:\n",
    "                print('Iter %d val loss = %.3f , time = %.2f' % (batch_idx, local_loss/50.,\n",
    "                                                                  (time.time() - start_time)/rgb_input.shape[0]))\n",
    "                local_loss = 0.0\n",
    "            \n",
    "            if _config['loss'] == 'points_distance' or _config['loss'] == 'combined':\n",
    "                total_val_loss += loss['total_loss'].item() * len(sample['rgb'])\n",
    "            \n",
    "            else : \n",
    "                total_val_loss += loss.item() * len(sample['rgb'])\n",
    "            \n",
    "            val_iter += 1\n",
    "\n",
    "        print(\"------------------------------------\")\n",
    "        print('total val loss = %.3f' % (total_val_loss / len(dataset_val)))\n",
    "        print(f'total traslation error: {total_val_t / len(dataset_val)} cm')\n",
    "        print(f'total rotation error: {total_val_r / len(dataset_val)} °')\n",
    "        print(\"------------------------------------\")\n",
    "\n",
    "        _run.log_scalar(\"Val_Loss\", total_val_loss / len(dataset_val), epoch)\n",
    "        _run.log_scalar(\"Val_t_error\", total_val_t / len(dataset_val), epoch)\n",
    "        _run.log_scalar(\"Val_r_error\", total_val_r / len(dataset_val), epoch)\n",
    "\n",
    "   \n",
    "\n",
    "     # SAVE\n",
    "        val_loss = total_val_loss / len(dataset_val)\n",
    "        if epoch % 1 == 0 :\n",
    "        #if val_loss < BEST_VAL_LOSS:\n",
    "        #    BEST_VAL_LOSS = val_loss\n",
    "            #_run.result = BEST_VAL_LOSS\n",
    "            if _config['rescale_transl'] > 0:\n",
    "                _run.result = total_val_t / len(dataset_val)\n",
    "            else:\n",
    "                _run.result = total_val_t / len(dataset_val)\n",
    "                #_run.result = total_val_r / len(dataset_val)\n",
    "            savefilename = f'{model_savepath}/checkpoint_r{_config[\"max_r\"]:.2f}_t{_config[\"max_t\"]:.2f}_e{epoch}_{val_loss:.3f}.tar'\n",
    "            torch.save({\n",
    "                'config': _config,\n",
    "                'epoch': epoch,\n",
    "                # 'state_dict': model.state_dict(), # single gpu\n",
    "                'state_dict': model.module.state_dict(), # multi gpu\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'train_loss': total_train_loss / len(dataset_train),\n",
    "                'val_loss': total_val_loss / len(dataset_val),\n",
    "            }, savefilename)\n",
    "            print(f'Model saved as {savefilename}')\n",
    "            if old_save_filename is not None:\n",
    "                if os.path.exists(old_save_filename):\n",
    "                    os.remove(old_save_filename)\n",
    "            old_save_filename = savefilename\n",
    "\n",
    "    print('full training time = %.2f HR' % ((time.time() - start_full_time) / 3600))\n",
    "    return _run.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "interior-change",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - LCCNet - No observers have been added to this run\n",
      "INFO - LCCNet - Running command 'main'\n",
      "INFO - LCCNet - Started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Function Choice: combined\n",
      "Val Sequence:  23\n",
      "number of kp =  500\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 11.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 12.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 13.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 14.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 15.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 16.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 17.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 18.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 19.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 20.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 21.\n",
      "number of kp =  500\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 11.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 12.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 13.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 14.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 15.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 16.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 17.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 18.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 19.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 20.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 21.\n",
      "VAL SET: Using this file: /mnt/data/kitti_odometry/data_odometry_valRT/dataset/sequences/val_RT_left_seq23_7.50_0.20.csv\n",
      "Number of the train dataset: 43552\n",
      "Number of the val dataset: 0\n",
      "log_savepath : ./checkpoints/kitti/odom/val_seq_23/log\n",
      "batch_size= 7\n",
      "6222\n",
      "0\n",
      "resnet # of input_images\n",
      "monodepth pre-trained weights freeze\n",
      "using lin_sine as positional encoding\n",
      "using lin_sine as positional encoding\n",
      "Number of model parameters: 53604677\n",
      "This is 0-th epoch\n",
      "Init worker 0 with seed 106876896\n",
      "Init worker 1 with seed 106876897\n",
      "Init worker 2 with seed 106876898\n",
      "Init worker 3 with seed 106876899\n",
      "Init worker 4 with seed 106876900\n",
      "Init worker 5 with seed 106876901\n",
      "Init worker 6 with seed 106876902\n",
      "Init worker 7 with seed 106876903\n",
      "Init worker 8 with seed 106876904\n",
      "Init worker 9 with seed 106876905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - LCCNet - Aborted after 0:00:11!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-106ea74a54b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/sacred/experiment.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, command_name, config_updates, named_configs, info, meta_info, options)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mcommand_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_updates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamed_configs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         )\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/sacred/run.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_heartbeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_pre_run_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_post_run_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/sacred/config/captured_function.py\u001b[0m in \u001b[0;36mcaptured_function\u001b[0;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# =================== run actual function =================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mConfigError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;31m# =========================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4950f7e7a4c3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_config, _run, seed)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mtime_for_50ep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainImgLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'batch {batch_idx+1}/{len(TrainImgLoader)}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ex.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
