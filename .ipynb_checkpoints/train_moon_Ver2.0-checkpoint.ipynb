{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "threaded-grain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot use vispy, setting triangulate_corr as None\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Copyright (C) 2020 Harbin Institute of Technology, China\n",
    "# Author: Xudong Lv (15B901019@hit.edu.cn)\n",
    "# Released under Creative Commons\n",
    "# Attribution-NonCommercial-ShareAlike 4.0 International License.\n",
    "# http://creativecommons.org/licenses/by-nc-sa/4.0/\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# import apex\n",
    "import mathutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as tvtf\n",
    "\n",
    "from sacred import Experiment\n",
    "from sacred.utils import apply_backspaces_and_linefeeds\n",
    "\n",
    "from DatasetLidarCamera import DatasetLidarCameraKittiOdometry\n",
    "from losses import DistancePoints3D, GeometricLoss, L1Loss, ProposedLoss, CombinedLoss\n",
    "\n",
    "\n",
    "from quaternion_distances import quaternion_distance\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from utils import (mat2xyzrpy, merge_inputs, overlay_imgs, quat2mat,\n",
    "                   quaternion_from_matrix, rotate_back, rotate_forward,\n",
    "                   tvector2mat)\n",
    "\n",
    "from LCCNet_COTR_moon_Ver2 import LCCNet\n",
    "from COTR.inference.sparse_engine import SparseEngine\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "ex = Experiment(\"LCCNet\" , interactive = True)\n",
    "ex.captured_out_filter = apply_backspaces_and_linefeeds\n",
    "from sacred import SETTINGS \n",
    "SETTINGS.CONFIG.READ_ONLY_CONFIG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "voluntary-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### dataset path root ############ \n",
    "\"\"\"\n",
    "root_dir = \"/mnt/data/kitti_odometry\"\n",
    "calib_path =\"data_odometry_calib\"\n",
    "image_path =\"data_odometry_color\"\n",
    "velodyne_path = \"data_odometry_velodyne\"\n",
    "imagegray_path = \"data_odometry_gray\"\n",
    "poses_path = \"data_odometry_poses\"\n",
    "\"\"\"\n",
    "#######################################\n",
    "# noinspection PyUnusedLocal\n",
    "@ex.config\n",
    "def config():\n",
    "    checkpoints = './checkpoints/'\n",
    "    dataset = 'kitti/odom' # 'kitti/raw'\n",
    "    data_folder = \"/mnt/data/kitti_odometry\"\n",
    "    use_reflectance = False\n",
    "    val_sequence = 0\n",
    "    epochs = 10\n",
    "    BASE_LEARNING_RATE = 3e-5 # 1e-4\n",
    "    loss = 'combined'\n",
    "    max_t = 1.0 # 1.5, 1.0,  0.5,  0.2,  0.1\n",
    "    max_r = 10.0 # 20.0, 10.0, 5.0,  2.0,  1.0\n",
    "    batch_size = 1  # 120\n",
    "    num_worker = 8\n",
    "    network = 'Res_f1'\n",
    "    optimizer = 'adam'\n",
    "    resume = True\n",
    "    #weights = './pretrained/kitti/kitti_iter5.tar'\n",
    "    #weights = './COTR/out/default/checkpoint.pth.tar'\n",
    "    weights = '/root/work/LCCNet_Moon/checkpoints/kitti/odom/val_seq_00/models/checkpoint_r10.00_t1.00_e0_3.526.tar'\n",
    "    #weights = None\n",
    "    rescale_rot = 1.0\n",
    "    rescale_transl = 2.0\n",
    "    precision = \"O0\"\n",
    "    norm = 'bn'\n",
    "    dropout = 0.0\n",
    "    max_depth = 80.\n",
    "    weight_point_cloud = 0.5\n",
    "    log_frequency = 10\n",
    "    print_frequency = 50\n",
    "    starting_epoch = 0\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1, 2, 3'\n",
    "\n",
    "EPOCH = 1\n",
    "def _init_fn(worker_id, seed):\n",
    "    seed = seed + worker_id + EPOCH*100\n",
    "    print(f\"Init worker {worker_id} with seed {seed}\")\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def get_2D_lidar_projection(pcl, cam_intrinsic):\n",
    "    pcl_xyz = cam_intrinsic @ pcl.T\n",
    "    pcl_xyz = pcl_xyz.T\n",
    "    pcl_z = pcl_xyz[:, 2]\n",
    "    pcl_xyz = pcl_xyz / (pcl_xyz[:, 2, None] + 1e-10)\n",
    "    pcl_uv = pcl_xyz[:, :2]\n",
    "\n",
    "    return pcl_uv, pcl_z\n",
    "\n",
    "def lidar_project_depth(pc_rotated, cam_calib, img_shape):\n",
    "    pc_rotated = pc_rotated[:3, :].detach().cpu().numpy()\n",
    "    cam_intrinsic = cam_calib.numpy()\n",
    "    pcl_uv, pcl_z = get_2D_lidar_projection(pc_rotated.T, cam_intrinsic)\n",
    "    mask = (pcl_uv[:, 0] > 0) & (pcl_uv[:, 0] < img_shape[1]) & (pcl_uv[:, 1] > 0) & (\n",
    "            pcl_uv[:, 1] < img_shape[0]) & (pcl_z > 0)\n",
    "    pcl_uv = pcl_uv[mask]\n",
    "    pcl_z = pcl_z[mask]\n",
    "    pcl_uv = pcl_uv.astype(np.uint32)\n",
    "    pcl_z = pcl_z.reshape(-1, 1)\n",
    "    depth_img = np.zeros((img_shape[0], img_shape[1], 1))\n",
    "    depth_img[pcl_uv[:, 1], pcl_uv[:, 0]] = pcl_z\n",
    "    depth_img = torch.from_numpy(depth_img.astype(np.float32))\n",
    "    depth_img = depth_img.cuda()\n",
    "    depth_img = depth_img.permute(2, 0, 1)\n",
    "\n",
    "    return depth_img, pcl_uv\n",
    "\n",
    "def two_images_side_by_side(img_a, img_b):\n",
    "    assert img_a.shape == img_b.shape, f'{img_a.shape} vs {img_b.shape}'\n",
    "    assert img_a.dtype == img_b.dtype\n",
    "    b, h, w, c = img_a.shape\n",
    "    canvas = np.zeros((b, h, 2 * w, c), dtype=img_a.cpu().numpy().dtype)\n",
    "    canvas[:, :, 0 * w:1 * w, :] = img_a.cpu().numpy()\n",
    "    canvas[:, :, 1 * w:2 * w, :] = img_b.cpu().numpy()\n",
    "    #canvas[:, :, : , 0 * w:1 * w] = img_a.cpu().numpy()\n",
    "    #canvas[:, :, : , 1 * w:2 * w] = img_b.cpu().numpy()\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "primary-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN training\n",
    "@ex.capture\n",
    "def train(model, optimizer, rgb_input, lidar_input, target_transl, target_rot, loss_fn, point_clouds, loss):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Run model\n",
    "    #transl_err, rot_err = model(rgb_img, refl_img)\n",
    "    transl_err, rot_err = model(rgb_input, lidar_input)\n",
    "    \"\"\"\n",
    "    print(\"transl_err==========\",transl_err)\n",
    "    print(\"rot_err=============\",rot_err)\n",
    "    print(\"point_clouds=============\",point_clouds)\n",
    "    print(\"target_transl=============\",target_transl)\n",
    "    print(\"target_rot=============\",target_rot)\n",
    "    \"\"\"\n",
    "    if loss == 'points_distance' or loss == 'combined':\n",
    "        losses = loss_fn(point_clouds, target_transl, target_rot, transl_err, rot_err)\n",
    "    else:\n",
    "        losses = loss_fn(target_transl, target_rot, transl_err, rot_err)\n",
    "    \n",
    "    #print(\"losses=============\",losses)\n",
    "\n",
    "    #losses['total_loss'].backward()\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return losses, rot_err, transl_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "copyrighted-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN test\n",
    "@ex.capture\n",
    "def val(model, rgb_input, lidar_input, target_transl, target_rot, loss_fn, point_clouds, loss):\n",
    "    model.eval()\n",
    "\n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        #transl_err, rot_err = model(rgb_img, refl_img)\n",
    "        transl_err, rot_err = model(rgb_input, lidar_input)\n",
    "\n",
    "    if loss == 'points_distance' or loss == 'combined':\n",
    "        losses = loss_fn(point_clouds, target_transl, target_rot, transl_err, rot_err)\n",
    "    else:\n",
    "        losses = loss_fn(target_transl, target_rot, transl_err, rot_err)\n",
    "\n",
    "    # if loss != 'points_distance':\n",
    "    #     total_loss = loss_fn(target_transl, target_rot, transl_err, rot_err)\n",
    "    # else:\n",
    "    #     total_loss = loss_fn(point_clouds, target_transl, target_rot, transl_err, rot_err)\n",
    "\n",
    "    total_trasl_error = torch.tensor(0.0).cuda()\n",
    "    target_transl = torch.tensor(target_transl).cuda()\n",
    "    #total_rot_error = quaternion_distance(target_rot, rot_err, target_rot.device)\n",
    "    #total_rot_error = total_rot_error * 180. / math.pi\n",
    "    for j in range(rgb_input.shape[0]):\n",
    "        total_trasl_error += torch.norm(target_transl[j] - transl_err[j]) * 100.\n",
    "\n",
    "    # # output image: The overlay image of the input rgb image and the projected lidar pointcloud depth image\n",
    "    # cam_intrinsic = camera_model[0]\n",
    "    # rotated_point_cloud =\n",
    "    # R_predicted = quat2mat(R_predicted[0])\n",
    "    # T_predicted = tvector2mat(T_predicted[0])\n",
    "    # RT_predicted = torch.mm(T_predicted, R_predicted)\n",
    "    # rotated_point_cloud = rotate_forward(rotated_point_cloud, RT_predicted)\n",
    "\n",
    "    #return losses, total_trasl_error.item(), total_rot_error.sum().item(), rot_err, transl_err\n",
    "    return losses, total_trasl_error.item(), rot_err, transl_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prerequisite-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ex.automain\n",
    "@ex.main\n",
    "def main(_config, _run, seed):\n",
    "    global EPOCH\n",
    "    print('Loss Function Choice: {}'.format(_config['loss']))\n",
    "\n",
    "    if _config['val_sequence'] is None:\n",
    "        raise TypeError('val_sequences cannot be None')\n",
    "    else:\n",
    "        _config['val_sequence'] = f\"{_config['val_sequence']:02d}\"\n",
    "        print(\"Val Sequence: \", _config['val_sequence'])\n",
    "        dataset_class = DatasetLidarCameraKittiOdometry\n",
    "    img_shape = (384, 1280) # 네트워크의 입력 규모\n",
    "    input_size = (256, 512)\n",
    "    _config[\"checkpoints\"] = os.path.join(_config[\"checkpoints\"], _config['dataset'])\n",
    "\n",
    "    dataset_train = dataset_class(_config['data_folder'], max_r=_config['max_r'], max_t=_config['max_t'],\n",
    "                                  split='train', use_reflectance=_config['use_reflectance'],\n",
    "                                  val_sequence=_config['val_sequence'])\n",
    "    dataset_val = dataset_class(_config['data_folder'], max_r=_config['max_r'], max_t=_config['max_t'],\n",
    "                                split='val', use_reflectance=_config['use_reflectance'],\n",
    "                                val_sequence=_config['val_sequence'])\n",
    "    model_savepath = os.path.join(_config['checkpoints'], 'val_seq_' + _config['val_sequence'], 'models')\n",
    "    if not os.path.exists(model_savepath):\n",
    "        os.makedirs(model_savepath)\n",
    "    log_savepath = os.path.join(_config['checkpoints'], 'val_seq_' + _config['val_sequence'], 'log')\n",
    "    if not os.path.exists(log_savepath):\n",
    "        os.makedirs(log_savepath)\n",
    "    train_writer = SummaryWriter(os.path.join(log_savepath, 'train'))\n",
    "    val_writer = SummaryWriter(os.path.join(log_savepath, 'val'))\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "\n",
    "    def init_fn(x): return _init_fn(x, seed)\n",
    "\n",
    "    train_dataset_size = len(dataset_train)\n",
    "    val_dataset_size = len(dataset_val)\n",
    "    print('Number of the train dataset: {}'.format(train_dataset_size))\n",
    "    print('Number of the val dataset: {}'.format(val_dataset_size))\n",
    "\n",
    "    # Training and validation set creation\n",
    "    num_worker = _config['num_worker']\n",
    "    batch_size = _config['batch_size']\n",
    "    print(\"batch_size=\" , batch_size)\n",
    "    TrainImgLoader = torch.utils.data.DataLoader(dataset=dataset_train,\n",
    "                                                 shuffle=True,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 num_workers=num_worker,\n",
    "                                                 worker_init_fn=init_fn,\n",
    "                                                 collate_fn=merge_inputs,\n",
    "                                                 drop_last=False,\n",
    "                                                 pin_memory=True)\n",
    "\n",
    "    ValImgLoader = torch.utils.data.DataLoader(dataset=dataset_val,\n",
    "                                                shuffle=False,\n",
    "                                                batch_size=batch_size,\n",
    "                                                num_workers=num_worker,\n",
    "                                                worker_init_fn=init_fn,\n",
    "                                                collate_fn=merge_inputs,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True)\n",
    "\n",
    "    print(len(TrainImgLoader))\n",
    "    print(len(ValImgLoader))\n",
    "\n",
    "    # loss function choice\n",
    "    if _config['loss'] == 'simple':\n",
    "        loss_fn = ProposedLoss(_config['rescale_transl'], _config['rescale_rot'])\n",
    "    elif _config['loss'] == 'geometric':\n",
    "        loss_fn = GeometricLoss()\n",
    "        loss_fn = loss_fn.cuda()\n",
    "    elif _config['loss'] == 'points_distance':\n",
    "        loss_fn = DistancePoints3D()\n",
    "    elif _config['loss'] == 'L1':\n",
    "        loss_fn = L1Loss(_config['rescale_transl'], _config['rescale_rot'])\n",
    "    elif _config['loss'] == 'combined':\n",
    "        loss_fn = CombinedLoss(_config['rescale_transl'], _config['rescale_rot'], _config['weight_point_cloud'])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown Loss Function\")\n",
    "\n",
    "    #runs = datetime.now().strftime('%b%d_%H-%M-%S') + \"/\"\n",
    "    # train_writer = SummaryWriter('./logs/' + runs)\n",
    "    #ex.info[\"tensorflow\"] = {}\n",
    "    #ex.info[\"tensorflow\"][\"logdirs\"] = ['./logs/' + runs]\n",
    "    \n",
    "    # Todo : 여기서 부터 잘 고쳐야함 !!!\n",
    "    # network choice and settings\n",
    "    if _config['network'].startswith('Res'):\n",
    "        feat = 1\n",
    "        md = 4\n",
    "        split = _config['network'].split('_')\n",
    "        for item in split[1:]:\n",
    "            if item.startswith('f'):\n",
    "                feat = int(item[-1])\n",
    "            elif item.startswith('md'):\n",
    "                md = int(item[2:])\n",
    "        assert 0 < feat < 7, \"Feature Number from PWC have to be between 1 and 6\"\n",
    "        assert 0 < md, \"md must be positive\"   \n",
    "        ### netwrok define ####\n",
    "        model = LCCNet(input_size, use_feat_from=feat, md=md,\n",
    "                         use_reflectance=_config['use_reflectance'], dropout=_config['dropout'],\n",
    "                         Action_Func='leakyrelu', attention=False, res_num=18)\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\"Network unknown\")\n",
    "    if _config['weights'] is not None:\n",
    "        print(f\"Loading weights from {_config['weights']}\")\n",
    "        checkpoint = torch.load(_config['weights'], map_location='cpu')\n",
    "        print('-------checkpoint-keys-------',checkpoint.keys() )\n",
    "        saved_state_dict = checkpoint['state_dict']\n",
    "        model.load_state_dict(saved_state_dict)\n",
    "\n",
    "        # original saved file with DataParallel\n",
    "        # state_dict = torch.load(model_path)\n",
    "        # create new OrderedDict that does not contain `module.`\n",
    "        # from collections import OrderedDict\n",
    "        # new_state_dict = OrderedDict()\n",
    "        # for k, v in checkpoint['state_dict'].items():\n",
    "        #     name = k[7:]  # remove `module.`\n",
    "        #     new_state_dict[name] = v\n",
    "        # # load params\n",
    "        # model.load_state_dict(new_state_dict)\n",
    "\n",
    "    # model = model.to(device)\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.cuda()\n",
    "\n",
    "    print('Number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n",
    "\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    if _config['loss'] == 'geometric':\n",
    "        parameters += list(loss_fn.parameters())\n",
    "    if _config['optimizer'] == 'adam':\n",
    "        optimizer = optim.Adam(parameters, lr=_config['BASE_LEARNING_RATE'], weight_decay=5e-6)\n",
    "        # Probably this scheduler is not used\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 50, 70], gamma=0.5)\n",
    "    else:\n",
    "        optimizer = optim.SGD(parameters, lr=_config['BASE_LEARNING_RATE'], momentum=0.9,\n",
    "                              weight_decay=5e-6, nesterov=True)\n",
    "    \n",
    "    starting_epoch = _config['starting_epoch']\n",
    "    if _config['weights'] is not None and _config['resume']:\n",
    "        checkpoint = torch.load(_config['weights'], map_location='cpu')\n",
    "        opt_state_dict = checkpoint['optimizer']\n",
    "        optimizer.load_state_dict(opt_state_dict)\n",
    "        if starting_epoch != 0:\n",
    "            starting_epoch = checkpoint['epoch']\n",
    "\n",
    "    # Allow mixed-precision if needed\n",
    "    # model, optimizer = apex.amp.initialize(model, optimizer, opt_level=_config[\"precision\"])\n",
    "\n",
    "    start_full_time = time.time()\n",
    "    BEST_VAL_LOSS = 10000.\n",
    "    old_save_filename = None\n",
    "\n",
    "    train_iter = 0\n",
    "    val_iter = 0\n",
    "    for epoch in range(starting_epoch, _config['epochs'] + 1):\n",
    "        EPOCH = epoch\n",
    "        print('This is %d-th epoch' % epoch)\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        total_train_loss = 0\n",
    "        local_loss = 0.\n",
    "        total_val_loss = 0.\n",
    "        total_val_t = 0.\n",
    "        total_val_r = 0.\n",
    "        \n",
    "        if _config['optimizer'] != 'adam':\n",
    "            _run.log_scalar(\"LR\", _config['BASE_LEARNING_RATE'] *\n",
    "                            math.exp((1 - epoch) * 4e-2), epoch)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = _config['BASE_LEARNING_RATE'] * \\\n",
    "                                    math.exp((1 - epoch) * 4e-2)\n",
    "        else:\n",
    "            #scheduler.step(epoch%100)\n",
    "            _run.log_scalar(\"LR\", scheduler.get_lr()[0])\n",
    "        \n",
    "                \n",
    "        ## Training ##\n",
    "        time_for_50ep = time.time()\n",
    "        # 실질적인 물리적인 의미의 data input preprocessing 작업\n",
    "        for batch_idx, sample in enumerate(TrainImgLoader):\n",
    "            #print(f'batch {batch_idx+1}/{len(TrainImgLoader)}', end='\\r')\n",
    "            start_time = time.time()\n",
    "            lidar_input = []\n",
    "            rgb_input = []\n",
    "            lidar_gt = []\n",
    "            shape_pad_input = []\n",
    "            real_shape_input = []\n",
    "            pc_rotated_input = []\n",
    "\n",
    "            # gt pose\n",
    "            sample['tr_error'] = sample['tr_error'].cuda()\n",
    "            sample['rot_error'] = sample['rot_error'].cuda()\n",
    "\n",
    "            start_preprocess = time.time()\n",
    "            for idx in range(len(sample['rgb'])):\n",
    "                # ProjectPointCloud in RT-pose\n",
    "                real_shape = [sample['rgb'][idx].shape[1], sample['rgb'][idx].shape[2], sample['rgb'][idx].shape[0]]\n",
    "\n",
    "                sample['point_cloud'][idx] = sample['point_cloud'][idx].cuda() # 카메라 좌표계로 변환 된 Lidar 포인트 클라우드\n",
    "                pc_lidar = sample['point_cloud'][idx].clone()\n",
    "\n",
    "                if _config['max_depth'] < 80.:\n",
    "                    pc_lidar = pc_lidar[:, pc_lidar[0, :] < _config['max_depth']].clone()\n",
    "\n",
    "                depth_gt, uv = lidar_project_depth(pc_lidar, sample['calib'][idx], real_shape) # image_shape\n",
    "                depth_gt /= _config['max_depth']\n",
    "\n",
    "                R = mathutils.Quaternion(sample['rot_error'][idx]).to_matrix()\n",
    "                R.resize_4x4()\n",
    "                T = mathutils.Matrix.Translation(sample['tr_error'][idx])\n",
    "                #RT = T * R\n",
    "                RT = T @ R # version change matutils * --> @ \n",
    "\n",
    "                pc_rotated = rotate_back(sample['point_cloud'][idx], RT) # Pc` = RT * Pc\n",
    "\n",
    "                if _config['max_depth'] < 80.:\n",
    "                    pc_rotated = pc_rotated[:, pc_rotated[0, :] < _config['max_depth']].clone()\n",
    "\n",
    "                depth_img, uv = lidar_project_depth(pc_rotated, sample['calib'][idx], real_shape) # image_shape\n",
    "                depth_img /= _config['max_depth']\n",
    "\n",
    "                # PAD ONLY ON RIGHT AND BOTTOM SIDE\n",
    "                rgb = sample['rgb'][idx].cuda()\n",
    "                shape_pad = [0, 0, 0, 0]\n",
    "\n",
    "                shape_pad[3] = (img_shape[0] - rgb.shape[1])  # // 2\n",
    "                shape_pad[1] = (img_shape[1] - rgb.shape[2])  # // 2 + 1\n",
    "\n",
    "                rgb = F.pad(rgb, shape_pad)\n",
    "                depth_img = F.pad(depth_img, shape_pad)\n",
    "                depth_gt = F.pad(depth_gt, shape_pad)\n",
    "                \n",
    "                #print (\"----------depth_img.shape---------\" , depth_img.shape)\n",
    "                \n",
    "                rgb = rgb.permute(1,2,0)\n",
    "                depth_img_np = depth_img.permute(1,2,0)\n",
    "                depth_gt_np = depth_gt.permute(1,2,0)\n",
    "                \n",
    "                rgb_np = rgb.cpu().numpy()\n",
    "                depth_img_np = depth_img_np.cpu().numpy()\n",
    "                depth_gt_np = depth_gt_np.cpu().numpy()\n",
    "                \n",
    "                #print (\"----------rgb_np.shape---------\" , rgb_np.shape)\n",
    "                #print (\"----------depth_img_np.shape---------\" , depth_img_np.shape)\n",
    "                \n",
    "                #display raw image#\n",
    "                \"\"\"\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                plt.subplot(211)\n",
    "                plt.imshow(rgb_np)\n",
    "                plt.title(\"RGB\", fontsize=22)\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(212)\n",
    "                plt.imshow(depth_img_np, cmap='magma')\n",
    "                plt.title(\"Lidar \", fontsize=22)\n",
    "                plt.axis('off');                \n",
    "                \"\"\"\n",
    "                rgb_np_resized = cv2.resize(rgb_np, (640,192), interpolation=cv2.INTER_LINEAR)\n",
    "                depth_img_np_resized = cv2.resize(depth_img_np, (640,192), interpolation=cv2.INTER_LINEAR)\n",
    "                depth_gt_np_resized = cv2.resize(depth_gt_np, (640,192), interpolation=cv2.INTER_LINEAR)\n",
    "                \n",
    "                #display resized image#\n",
    "                \"\"\"\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                plt.subplot(211)\n",
    "                plt.imshow(rgb_np_resized)\n",
    "                plt.title(\"RGB_resize\", fontsize=22)\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(212)\n",
    "                plt.imshow(depth_img_np_resized, cmap='magma')\n",
    "                plt.title(\"Lidar_resize\", fontsize=22)\n",
    "                plt.axis('off');                  \n",
    "                \"\"\"\n",
    "                #print (\"----------after resazing rgb_np.shape---------\" , rgb_np.shape)\n",
    "                #print (\"----------after resazing depth_img_np.shape---------\" , depth_img_np.shape)          \n",
    "                rgb_np_resized_color = rgb_np_resized\n",
    "                depth_img_np_resized_color = cv2.cvtColor(depth_img_np_resized, cv2.COLOR_GRAY2RGB)\n",
    "                depth_gt_np_resized_color = cv2.cvtColor(depth_gt_np_resized, cv2.COLOR_GRAY2RGB)\n",
    "#                 print (\"----------rgb_np_resized_color.shape---------------\" , rgb_np_resized_color.shape)\n",
    "#                 print (\"----------depth_img_np_resized_color.shape---------\" , depth_img_np_resized_color.shape)     \n",
    "\n",
    "                #display gray to color#\n",
    "                \"\"\"\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                plt.subplot(211)\n",
    "                plt.imshow(rgb_np_resized)\n",
    "                plt.title(\"RGB_resize\", fontsize=22)\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(212)\n",
    "                plt.imshow(depth_img_np_resized_color, cmap='magma')\n",
    "                plt.title(\"Lidar_resize_color\", fontsize=22)\n",
    "                plt.axis('off');                     \n",
    "                \"\"\"\n",
    "                \n",
    "                input_rgb_pytorch = transforms.ToTensor()(rgb_np_resized_color)\n",
    "                input_lidar_pytorch = transforms.ToTensor()(depth_img_np_resized_color)\n",
    "                input_lidar_gt_pytorch = transforms.ToTensor()(depth_gt_np_resized_color)\n",
    "                \n",
    "                #print (\" --------- final input rgb.shape----------\" , input_rgb_pytorch.shape)\n",
    "                #print (\" --------- final input lidar.shape----------\" , input_lidar_pytorch.shape)\n",
    "                \n",
    "                #print (\"======= raw data display end================\")\n",
    "                \n",
    "#                 loss, R_predicted,  T_predicted = train(model, optimizer, input_rgb_pytorch, input_lidar_pytorch,\n",
    "#                                        sample['tr_error'], sample['rot_error'],\n",
    "#                                        loss_fn, sample['point_cloud'], _config['loss'])\n",
    "                \n",
    "                # batch stack \n",
    "                rgb_input.append(input_rgb_pytorch)\n",
    "                lidar_input.append(input_lidar_pytorch)\n",
    "                lidar_gt.append(input_lidar_gt_pytorch)\n",
    "                real_shape_input.append(real_shape)\n",
    "                shape_pad_input.append(shape_pad)\n",
    "                pc_rotated_input.append(pc_rotated)\n",
    "            \n",
    "            rgb_input = torch.stack(rgb_input)\n",
    "            lidar_input = torch.stack(lidar_input)\n",
    "            rgb_show = rgb_input.clone()\n",
    "            lidar_show = lidar_input.clone()\n",
    "\n",
    "            #rgb_input = F.interpolate(rgb_input, size=[256, 256], mode=\"bilinear\")\n",
    "            #lidar_input = F.interpolate(lidar_input, size=[256, 256], mode=\"bilinear\")\n",
    "            #print (\"---------rgb_input_shape--------\", rgb_input.shape)\n",
    "            #print (\"---------lidar_input_shape--------\", lidar_input.shape)\n",
    "            \n",
    "            #display for batch input\n",
    "            \"\"\"\n",
    "            rgb_input_np = rgb_input.permute(0,2,3,1)\n",
    "            lidar_input_np = lidar_input.permute(0,2,3,1)\n",
    "            rgb_input_np = rgb_input_np.cpu().numpy()\n",
    "            lidar_input_np = lidar_input_np.cpu().numpy()\n",
    "            \n",
    "            plt.imshow(rgb_input_np[0])\n",
    "            plt.show()\n",
    "            \n",
    "            plt.imshow(lidar_input_np[0])\n",
    "            plt.show()\n",
    "            #start of modify\n",
    "            \"\"\"\n",
    "            \n",
    "            \"\"\"\n",
    "            print (\"---------rgb_input_shape--------\", rgb_input.shape)\n",
    "            print (\"---------lidar_input_shape--------\", lidar_input.shape)\n",
    "            print (\"---------lidar_input_shape--------\", lidar_input_nested.shape)\n",
    "            print (\"---------nested_input_shape--------\", nested_input.shape)\n",
    "            print (\"---------queries_shape--------\", queries.shape)\n",
    "            \"\"\"\n",
    "            \n",
    "            end_preprocess = time.time()\n",
    "            #print(\"lidar_input===========\",lidar_input)\n",
    "            #print(\"rgb_input===========\",rgb_input)\n",
    "            #print(\"sample['tr_error']===========\",sample['tr_error'])\n",
    "            #print(\"sample['rot_error']===========\",sample['rot_error'])\n",
    "            loss, R_predicted,  T_predicted = train(model, optimizer, rgb_input, lidar_input,\n",
    "                                                  sample['tr_error'], sample['rot_error'],\n",
    "                                                  loss_fn, sample['point_cloud'], _config['loss'])\n",
    "            \"\"\"\n",
    "            for key in loss.keys():\n",
    "                if loss[key].item() != loss[key].item():\n",
    "                    raise ValueError(\"Loss {} is NaN\".format(key))\n",
    "            \"\"\"\n",
    "            \n",
    "            if batch_idx % _config['log_frequency'] == 0:\n",
    "                \"\"\"\n",
    "                show_idx = 0\n",
    "                # output image: The overlay image of the input rgb image\n",
    "                # and the projected lidar pointcloud depth image\n",
    "                \n",
    "                rotated_point_cloud = pc_rotated_input[show_idx]\n",
    "                R_predicted = quat2mat(R_predicted[show_idx])\n",
    "                T_predicted = tvector2mat(T_predicted[show_idx])\n",
    "                RT_predicted = torch.mm(T_predicted, R_predicted)\n",
    "                rotated_point_cloud = rotate_forward(rotated_point_cloud, RT_predicted)\n",
    "\n",
    "                depth_pred, uv = lidar_project_depth(rotated_point_cloud,\n",
    "                                                    sample['calib'][show_idx],\n",
    "                                                    real_shape_input[show_idx]) # or image_shape\n",
    "                depth_pred /= _config['max_depth']\n",
    "                depth_pred = F.pad(depth_pred, shape_pad_input[show_idx])\n",
    "\n",
    "                pred_show = overlay_imgs(rgb_show[show_idx], depth_pred.unsqueeze(0))\n",
    "                input_show = overlay_imgs(rgb_show[show_idx], lidar_show[show_idx].unsqueeze(0))\n",
    "                gt_show = overlay_imgs(rgb_show[show_idx], lidar_gt[show_idx].unsqueeze(0))\n",
    "\n",
    "                pred_show = torch.from_numpy(pred_show)\n",
    "                pred_show = pred_show.permute(2, 0, 1)\n",
    "                input_show = torch.from_numpy(input_show)\n",
    "                input_show = input_show.permute(2, 0, 1)\n",
    "                gt_show = torch.from_numpy(gt_show)\n",
    "                gt_show = gt_show.permute(2, 0, 1)\n",
    "\n",
    "                train_writer.add_image(\"input_proj_lidar\", input_show, train_iter)\n",
    "                train_writer.add_image(\"gt_proj_lidar\", gt_show, train_iter)\n",
    "                train_writer.add_image(\"pred_proj_lidar\", pred_show, train_iter)\n",
    "\n",
    "                train_writer.add_scalar(\"Loss_Total\", loss.item(), train_iter)\n",
    "                #train_writer.add_scalar(\"Loss_Translation\", loss['transl_loss'].item(), train_iter)\n",
    "                #train_writer.add_scalar(\"Loss_Rotation\", loss['rot_loss'].item(), train_iter)\n",
    "                \"\"\"\n",
    "                if _config['loss'] == 'combined':\n",
    "                    train_writer.add_scalar(\"Loss_Point_clouds\", loss.item(), train_iter)\n",
    "\n",
    "            #local_loss += loss['total_loss'].item()\n",
    "            local_loss += loss.item()\n",
    "            train_local_loss = local_loss/50\n",
    "            \n",
    "            \n",
    "            if batch_idx % 50 == 0 and batch_idx != 0:\n",
    "\n",
    "                print(f'Iter {batch_idx}/{len(TrainImgLoader)} training loss = {local_loss/50:.3f}, '\n",
    "                      f'time = {(time.time() - start_time)/lidar_input.shape[0]:.4f}, '\n",
    "                      #f'time_preprocess = {(end_preprocess-start_preprocess)/lidar_input.shape[0]:.4f}, '\n",
    "                      f'time for 50 iter: {time.time()-time_for_50ep:.4f}')\n",
    "                time_for_50ep = time.time()\n",
    "                _run.log_scalar(\"Loss\", local_loss/50, train_iter)\n",
    "                local_loss = 0.\n",
    "                \n",
    "                #train_loss = train_local_loss / len(dataset_train)\n",
    "                ######### save network model for intermediate verification #####################  \n",
    "                if train_local_loss < 6.0:\n",
    "                    #if val_loss < BEST_VAL_LOSS:\n",
    "                #    BEST_VAL_LOSS = val_loss\n",
    "                    #_run.result = BEST_VAL_LOSS\n",
    "                    if _config['rescale_transl'] > 0:\n",
    "                        _run.result = total_val_t / len(dataset_val)\n",
    "                    else:\n",
    "                        _run.result = total_val_t / len(dataset_val)\n",
    "                        #_run.result = total_val_r / len(dataset_val)\n",
    "                    savefilename = f'{model_savepath}/checkpoint_r{_config[\"max_r\"]:.2f}_t{_config[\"max_t\"]:.2f}_e{epoch}_{train_local_loss:.3f}.tar'\n",
    "                    torch.save({\n",
    "                        'config': _config,\n",
    "                        'epoch': epoch,\n",
    "                        # 'state_dict': model.state_dict(), # single gpu\n",
    "                        'state_dict': model.module.state_dict(), # multi gpu\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'train_loss': total_train_loss / len(dataset_train),\n",
    "                        'val_loss': total_val_loss / len(dataset_val),\n",
    "                    }, savefilename)\n",
    "                    print(f'Model saved as {savefilename}')\n",
    "                    if old_save_filename is not None:\n",
    "                        if os.path.exists(old_save_filename):\n",
    "                            os.remove(old_save_filename)\n",
    "                    old_save_filename = savefilename                \n",
    "                    \n",
    "            #total_train_loss += loss['total_loss'].item() * len(sample['rgb'])\n",
    "            total_train_loss += loss.item() * len(sample['rgb'])\n",
    "            train_iter += 1\n",
    "            # total_iter += len(sample['rgb'])\n",
    "\n",
    "        print(\"------------------------------------\")\n",
    "        print('epoch %d total training loss = %.3f' % (epoch, total_train_loss / len(dataset_train)))\n",
    "        print('Total epoch time = %.2f' % (time.time() - epoch_start_time))\n",
    "        print(\"------------------------------------\")\n",
    "        _run.log_scalar(\"Total training loss\", total_train_loss / len(dataset_train), epoch)\n",
    "\n",
    "        ## Validation ##\n",
    "        total_val_loss = 0.\n",
    "        total_val_t = 0.\n",
    "        total_val_r = 0.\n",
    "\n",
    "        local_loss = 0.0\n",
    "        for batch_idx, sample in enumerate(ValImgLoader):\n",
    "            #print(f'batch {batch_idx+1}/{len(TrainImgLoader)}', end='\\r')\n",
    "            start_time = time.time()\n",
    "            lidar_input = []\n",
    "            rgb_input = []\n",
    "            lidar_gt = []\n",
    "            shape_pad_input = []\n",
    "            real_shape_input = []\n",
    "            pc_rotated_input = []\n",
    "\n",
    "            # gt pose\n",
    "            sample['tr_error'] = sample['tr_error'].cuda()\n",
    "            sample['rot_error'] = sample['rot_error'].cuda()\n",
    "\n",
    "            for idx in range(len(sample['rgb'])):\n",
    "                # ProjectPointCloud in RT-pose\n",
    "                real_shape = [sample['rgb'][idx].shape[1], sample['rgb'][idx].shape[2], sample['rgb'][idx].shape[0]]\n",
    "\n",
    "                sample['point_cloud'][idx] = sample['point_cloud'][idx].cuda() # 카메라 좌표계로의 Lidar 포인트 클라우드 변환\n",
    "                pc_lidar = sample['point_cloud'][idx].clone()\n",
    "\n",
    "                if _config['max_depth'] < 80.:\n",
    "                    pc_lidar = pc_lidar[:, pc_lidar[0, :] < _config['max_depth']].clone()\n",
    "\n",
    "                depth_gt, uv = lidar_project_depth(pc_lidar, sample['calib'][idx], real_shape) # image_shape\n",
    "                depth_gt /= _config['max_depth']\n",
    "\n",
    "                reflectance = None\n",
    "                if _config['use_reflectance']:\n",
    "                    reflectance = sample['reflectance'][idx].cuda()\n",
    "\n",
    "                R = mathutils.Quaternion(sample['rot_error'][idx]).to_matrix()\n",
    "                R.resize_4x4()\n",
    "                T = mathutils.Matrix.Translation(sample['tr_error'][idx])\n",
    "                #RT = T * R\n",
    "                RT = T @ R\n",
    "\n",
    "                pc_rotated = rotate_back(sample['point_cloud'][idx], RT) # Pc` = RT * Pc\n",
    "\n",
    "                if _config['max_depth'] < 80.:\n",
    "                    pc_rotated = pc_rotated[:, pc_rotated[0, :] < _config['max_depth']].clone()\n",
    "\n",
    "                depth_img, uv = lidar_project_depth(pc_rotated, sample['calib'][idx], real_shape) # image_shape\n",
    "                depth_img /= _config['max_depth']\n",
    "\n",
    "                if _config['use_reflectance']:\n",
    "                    # This need to be checked\n",
    "                    # cam_params = sample['calib'][idx].cuda()\n",
    "                    # cam_model = CameraModel()\n",
    "                    # cam_model.focal_length = cam_params[:2]\n",
    "                    # cam_model.principal_point = cam_params[2:]\n",
    "                    # uv, depth, _, refl = cam_model.project_pytorch(pc_rotated, real_shape, reflectance)\n",
    "                    # uv = uv.long()\n",
    "                    # indexes = depth_img[uv[:,1], uv[:,0]] == depth\n",
    "                    # refl_img = torch.zeros(real_shape[:2], device='cuda', dtype=torch.float)\n",
    "                    # refl_img[uv[indexes, 1], uv[indexes, 0]] = refl[0, indexes]\n",
    "                    refl_img = None\n",
    "\n",
    "                # if not _config['use_reflectance']:\n",
    "                #     depth_img = depth_img.unsqueeze(0)\n",
    "                # else:\n",
    "                #     depth_img = torch.stack((depth_img, refl_img))\n",
    "\n",
    "                # PAD ONLY ON RIGHT AND BOTTOM SIDE\n",
    "                rgb = sample['rgb'][idx].cuda()\n",
    "                shape_pad = [0, 0, 0, 0]\n",
    "\n",
    "                shape_pad[3] = (img_shape[0] - rgb.shape[1])  # // 2\n",
    "                shape_pad[1] = (img_shape[1] - rgb.shape[2])  # // 2 + 1\n",
    "\n",
    "                rgb = F.pad(rgb, shape_pad)\n",
    "                depth_img = F.pad(depth_img, shape_pad)\n",
    "                depth_gt = F.pad(depth_gt, shape_pad)\n",
    "\n",
    "                rgb = rgb.permute(1,2,0)\n",
    "                depth_img_np = depth_img.permute(1,2,0)\n",
    "                depth_gt_np = depth_gt.permute(1,2,0)\n",
    "                \n",
    "                rgb_np = rgb.cpu().numpy()\n",
    "                depth_img_np = depth_img_np.cpu().numpy()\n",
    "                depth_gt_np = depth_gt_np.cpu().numpy()\n",
    "                \n",
    "                rgb_np_resized = cv2.resize(rgb_np, (640,192), interpolation=cv2.INTER_LINEAR)\n",
    "                depth_img_np_resized = cv2.resize(depth_img_np, (640,192), interpolation=cv2.INTER_LINEAR)\n",
    "                depth_gt_np_resized = cv2.resize(depth_gt_np, (640,192), interpolation=cv2.INTER_LINEAR)\n",
    "                \n",
    "                rgb_np_resized_color = rgb_np_resized\n",
    "                depth_img_np_resized_color = cv2.cvtColor(depth_img_np_resized, cv2.COLOR_GRAY2RGB)\n",
    "                depth_gt_np_resized_color = cv2.cvtColor(depth_img_np_resized, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "                input_rgb_pytorch = transforms.ToTensor()(rgb_np_resized_color)\n",
    "                input_lidar_pytorch = transforms.ToTensor()(depth_img_np_resized_color)\n",
    "                input_lidar_gt_pytorch = transforms.ToTensor()(depth_gt_np_resized_color)\n",
    "\n",
    "#                 loss, trasl_e, R_predicted,  T_predicted = val(model, input_rgb_pytorch, input_lidar_pytorch,\n",
    "#                                                       sample['tr_error'], sample['rot_error'],\n",
    "#                                                       loss_fn, sample['point_cloud'], _config['loss'])\n",
    "                \n",
    "                rgb_input.append(input_rgb_pytorch)\n",
    "                lidar_input.append(input_lidar_pytorch)\n",
    "                lidar_gt.append(input_lidar_gt_pytorch)\n",
    "                real_shape_input.append(real_shape)\n",
    "                shape_pad_input.append(shape_pad)\n",
    "                pc_rotated_input.append(pc_rotated)\n",
    "\n",
    "            lidar_input = torch.stack(lidar_input)\n",
    "            rgb_input = torch.stack(rgb_input)\n",
    "            rgb_show = rgb_input.clone()\n",
    "            lidar_show = lidar_input.clone()\n",
    "#             rgb_input = F.interpolate(rgb_input, size=[256, 256], mode=\"bilinear\")\n",
    "#             lidar_input = F.interpolate(lidar_input, size=[256, 256], mode=\"bilinear\")\n",
    "            \n",
    "                        \n",
    "            loss, trasl_e, R_predicted,  T_predicted = val(model, rgb_input, lidar_input,\n",
    "                                                                  sample['tr_error'], sample['rot_error'],\n",
    "                                                                  loss_fn, sample['point_cloud'], _config['loss'])\n",
    "            \n",
    "            \"\"\"\n",
    "            for key in loss.keys():\n",
    "                if loss[key].item() != loss[key].item():\n",
    "                    raise ValueError(\"Loss {} is NaN\".format(key))\n",
    "            \"\"\"\n",
    "            \n",
    "            if batch_idx % _config['log_frequency'] == 0:\n",
    "                \"\"\"\n",
    "                show_idx = 0\n",
    "                # output image: The overlay image of the input rgb image\n",
    "                # and the projected lidar pointcloud depth image\n",
    "                \n",
    "                rotated_point_cloud = pc_rotated_input[show_idx]\n",
    "                R_predicted = quat2mat(R_predicted[show_idx])\n",
    "                T_predicted = tvector2mat(T_predicted[show_idx])\n",
    "                RT_predicted = torch.mm(T_predicted, R_predicted)\n",
    "                rotated_point_cloud = rotate_forward(rotated_point_cloud, RT_predicted)\n",
    "\n",
    "                depth_pred, uv = lidar_project_depth(rotated_point_cloud,\n",
    "                                                    sample['calib'][show_idx],\n",
    "                                                    real_shape_input[show_idx]) # or image_shape\n",
    "                depth_pred /= _config['max_depth']\n",
    "                depth_pred = F.pad(depth_pred, shape_pad_input[show_idx])\n",
    "\n",
    "                pred_show = overlay_imgs(rgb_show[show_idx], depth_pred.unsqueeze(0))\n",
    "                input_show = overlay_imgs(rgb_show[show_idx], lidar_show[show_idx].unsqueeze(0))\n",
    "                gt_show = overlay_imgs(rgb_show[show_idx], lidar_gt[show_idx].unsqueeze(0))\n",
    "\n",
    "                pred_show = torch.from_numpy(pred_show)\n",
    "                pred_show = pred_show.permute(2, 0, 1)\n",
    "                input_show = torch.from_numpy(input_show)\n",
    "                input_show = input_show.permute(2, 0, 1)\n",
    "                gt_show = torch.from_numpy(gt_show)\n",
    "                gt_show = gt_show.permute(2, 0, 1)\n",
    "\n",
    "                val_writer.add_image(\"input_proj_lidar\", input_show, val_iter)\n",
    "                val_writer.add_image(\"gt_proj_lidar\", gt_show, val_iter)\n",
    "                val_writer.add_image(\"pred_proj_lidar\", pred_show, val_iter)\n",
    "\n",
    "                val_writer.add_scalar(\"Loss_Total\", loss.item(), val_iter)\n",
    "                #val_writer.add_scalar(\"Loss_Translation\", loss['transl_loss'].item(), val_iter)\n",
    "                #val_writer.add_scalar(\"Loss_Rotation\", loss['rot_loss'].item(), val_iter)\n",
    "                \"\"\"\n",
    "                if _config['loss'] == 'combined':\n",
    "                    val_writer.add_scalar(\"Loss_Point_clouds\", loss.item(), val_iter)\n",
    "\n",
    "\n",
    "            total_val_t += trasl_e\n",
    "            #total_val_r += rot_e\n",
    "            #local_loss += loss['total_loss'].item()\n",
    "            local_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 50 == 0 and batch_idx != 0:\n",
    "                print('Iter %d val loss = %.3f , time = %.2f' % (batch_idx, local_loss/50.,\n",
    "                                                                  (time.time() - start_time)/lidar_input.shape[0]))\n",
    "                local_loss = 0.0\n",
    "            #total_val_loss += loss['total_loss'].item() * len(sample['rgb'])\n",
    "            total_val_loss += loss.item() * len(sample['rgb'])\n",
    "            val_iter += 1\n",
    "\n",
    "        print(\"------------------------------------\")\n",
    "        print('total val loss = %.3f' % (total_val_loss / len(dataset_val)))\n",
    "        print(f'total traslation error: {total_val_t / len(dataset_val)} cm')\n",
    "        #print(f'total rotation error: {total_val_r / len(dataset_val)} °')\n",
    "        print(\"------------------------------------\")\n",
    "\n",
    "        _run.log_scalar(\"Val_Loss\", total_val_loss / len(dataset_val), epoch)\n",
    "        _run.log_scalar(\"Val_t_error\", total_val_t / len(dataset_val), epoch)\n",
    "        #_run.log_scalar(\"Val_r_error\", total_val_r / len(dataset_val), epoch)\n",
    "\n",
    "   \n",
    "\n",
    "     # SAVE\n",
    "        val_loss = total_val_loss / len(dataset_val)\n",
    "        if epoch % 1 == 0 :\n",
    "        #if val_loss < BEST_VAL_LOSS:\n",
    "        #    BEST_VAL_LOSS = val_loss\n",
    "            #_run.result = BEST_VAL_LOSS\n",
    "            if _config['rescale_transl'] > 0:\n",
    "                _run.result = total_val_t / len(dataset_val)\n",
    "            else:\n",
    "                _run.result = total_val_t / len(dataset_val)\n",
    "                #_run.result = total_val_r / len(dataset_val)\n",
    "            savefilename = f'{model_savepath}/checkpoint_r{_config[\"max_r\"]:.2f}_t{_config[\"max_t\"]:.2f}_e{epoch}_{val_loss:.3f}.tar'\n",
    "            torch.save({\n",
    "                'config': _config,\n",
    "                'epoch': epoch,\n",
    "                # 'state_dict': model.state_dict(), # single gpu\n",
    "                'state_dict': model.module.state_dict(), # multi gpu\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'train_loss': total_train_loss / len(dataset_train),\n",
    "                'val_loss': total_val_loss / len(dataset_val),\n",
    "            }, savefilename)\n",
    "            print(f'Model saved as {savefilename}')\n",
    "            if old_save_filename is not None:\n",
    "                if os.path.exists(old_save_filename):\n",
    "                    os.remove(old_save_filename)\n",
    "            old_save_filename = savefilename\n",
    "\n",
    "    print('full training time = %.2f HR' % ((time.time() - start_full_time) / 3600))\n",
    "    return _run.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "possible-complexity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - LCCNet - No observers have been added to this run\n",
      "INFO - LCCNet - Running command 'main'\n",
      "INFO - LCCNet - Started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Function Choice: combined\n",
      "Val Sequence:  00\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 11.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 12.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 13.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 14.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 15.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 16.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 17.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 18.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 19.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 20.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 21.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 11.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 12.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 13.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 14.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 15.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 16.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 17.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 18.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 19.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 20.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 21.\n",
      "VAL SET: Using this file: /mnt/data/kitti_odometry/data_odometry_valRT/dataset/sequences/val_RT_left_seq00_10.00_1.00.csv\n",
      "Number of the train dataset: 39011\n",
      "Number of the val dataset: 4541\n",
      "batch_size= 1\n",
      "39011\n",
      "4541\n",
      "using lin_sine as positional encoding\n",
      "freeze conv1.weight\n",
      "freeze layer1.0.conv1.weight\n",
      "freeze layer1.0.conv2.weight\n",
      "freeze layer1.0.conv3.weight\n",
      "freeze layer1.0.downsample.0.weight\n",
      "freeze layer1.1.conv1.weight\n",
      "freeze layer1.1.conv2.weight\n",
      "freeze layer1.1.conv3.weight\n",
      "freeze layer1.2.conv1.weight\n",
      "freeze layer1.2.conv2.weight\n",
      "freeze layer1.2.conv3.weight\n",
      "freeze layer2.0.conv1.weight\n",
      "freeze layer2.0.conv2.weight\n",
      "freeze layer2.0.conv3.weight\n",
      "freeze layer2.0.downsample.0.weight\n",
      "freeze layer2.1.conv1.weight\n",
      "freeze layer2.1.conv2.weight\n",
      "freeze layer2.1.conv3.weight\n",
      "freeze layer2.2.conv1.weight\n",
      "freeze layer2.2.conv2.weight\n",
      "freeze layer2.2.conv3.weight\n",
      "freeze layer2.3.conv1.weight\n",
      "freeze layer2.3.conv2.weight\n",
      "freeze layer2.3.conv3.weight\n",
      "freeze layer3.0.conv1.weight\n",
      "freeze layer3.0.conv2.weight\n",
      "freeze layer3.0.conv3.weight\n",
      "freeze layer3.0.downsample.0.weight\n",
      "freeze layer3.1.conv1.weight\n",
      "freeze layer3.1.conv2.weight\n",
      "freeze layer3.1.conv3.weight\n",
      "freeze layer3.2.conv1.weight\n",
      "freeze layer3.2.conv2.weight\n",
      "freeze layer3.2.conv3.weight\n",
      "freeze layer3.3.conv1.weight\n",
      "freeze layer3.3.conv2.weight\n",
      "freeze layer3.3.conv3.weight\n",
      "freeze layer3.4.conv1.weight\n",
      "freeze layer3.4.conv2.weight\n",
      "freeze layer3.4.conv3.weight\n",
      "freeze layer3.5.conv1.weight\n",
      "freeze layer3.5.conv2.weight\n",
      "freeze layer3.5.conv3.weight\n",
      "freeze layer4.0.conv1.weight\n",
      "freeze layer4.0.conv2.weight\n",
      "freeze layer4.0.conv3.weight\n",
      "freeze layer4.0.downsample.0.weight\n",
      "freeze layer4.1.conv1.weight\n",
      "freeze layer4.1.conv2.weight\n",
      "freeze layer4.1.conv3.weight\n",
      "freeze layer4.2.conv1.weight\n",
      "freeze layer4.2.conv2.weight\n",
      "freeze layer4.2.conv3.weight\n",
      "freeze fc.weight\n",
      "freeze fc.bias\n",
      "using lin_sine as positional encoding\n",
      "Loading weights from ./COTR/out/default/checkpoint.pth.tar\n",
      "weights safely loaded\n",
      "COTR pre-trained weights freeze\n",
      "Loading weights from /root/work/LCCNet_Moon/checkpoints/kitti/odom/val_seq_00/models/checkpoint_r10.00_t1.00_e0_3.526.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR - LCCNet - Failed after 0:00:03!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------checkpoint-keys------- dict_keys(['config', 'epoch', 'state_dict', 'optimizer', 'train_loss', 'val_loss'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-106ea74a54b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/sacred/experiment.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, command_name, config_updates, named_configs, info, meta_info, options)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mcommand_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_updates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamed_configs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         )\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/sacred/run.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_heartbeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_pre_run_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_post_run_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/sacred/config/captured_function.py\u001b[0m in \u001b[0;36mcaptured_function\u001b[0;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# =================== run actual function =================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mConfigError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;31m# =========================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-177b31c53995>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_config, _run, seed)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-------checkpoint-keys-------'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0msaved_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_state_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model_state_dict'"
     ]
    }
   ],
   "source": [
    "ex.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
