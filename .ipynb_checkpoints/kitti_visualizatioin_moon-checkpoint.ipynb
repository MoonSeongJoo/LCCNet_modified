{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "green-footage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot use vispy, setting triangulate_corr as None\n",
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Copyright (C) 2020 Harbin Institute of Technology, China\n",
    "# Author: Xudong Lv (15B901019@hit.edu.cn)\n",
    "# Released under Creative Commons\n",
    "# Attribution-NonCommercial-ShareAlike 4.0 International License.\n",
    "# http://creativecommons.org/licenses/by-nc-sa/4.0/\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# import apex\n",
    "import mathutils\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.transforms import functional as tvtf\n",
    "\n",
    "from sacred import Experiment\n",
    "from sacred.utils import apply_backspaces_and_linefeeds\n",
    "\n",
    "from DatasetLidarCamera_Ver6 import DatasetLidarCameraKittiOdometry\n",
    "from losses_Ver6 import DistancePoints3D, GeometricLoss, L1Loss, ProposedLoss, CombinedLoss\n",
    "\n",
    "\n",
    "from quaternion_distances import quaternion_distance\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from utils import (mat2xyzrpy, merge_inputs, overlay_imgs, quat2mat,\n",
    "                   quaternion_from_matrix, rotate_back, rotate_forward,\n",
    "                   tvector2mat)\n",
    "\n",
    "from LCCNet_COTR_moon_Ver6 import LCCNet\n",
    "from COTR.inference.sparse_engine import SparseEngine\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "ex = Experiment(\"LCCNet\" , interactive = True)\n",
    "ex.captured_out_filter = apply_backspaces_and_linefeeds\n",
    "from sacred import SETTINGS \n",
    "SETTINGS.CONFIG.READ_ONLY_CONFIG = False\n",
    "\n",
    "import open3d as o3\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "isolated-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### dataset path root ############ \n",
    "\"\"\"\n",
    "root_dir = \"/mnt/data/kitti_odometry\"\n",
    "calib_path =\"data_odometry_calib\"\n",
    "image_path =\"data_odometry_color\"\n",
    "velodyne_path = \"data_odometry_velodyne\"\n",
    "imagegray_path = \"data_odometry_gray\"\n",
    "poses_path = \"data_odometry_poses\"\n",
    "\"\"\"\n",
    "#######################################\n",
    "# noinspection PyUnusedLocal\n",
    "@ex.config\n",
    "def config():\n",
    "    checkpoints = './checkpoints/'\n",
    "    dataset = 'kitti/odom' # 'kitti/raw'\n",
    "    data_folder = \"/mnt/data/kitti_odometry\"\n",
    "    use_reflectance = False\n",
    "    val_sequence = 0\n",
    "    epochs = 10\n",
    "    BASE_LEARNING_RATE = 3e-5 # 1e-4\n",
    "    loss = 'combined'\n",
    "    max_t = 1.0 # 1.5, 1.0,  0.5,  0.2,  0.1\n",
    "    max_r = 10.0 # 20.0, 10.0, 5.0,  2.0,  1.0\n",
    "    batch_size = 1  # 120\n",
    "    num_worker = 8\n",
    "    network = 'Res_f1'\n",
    "    optimizer = 'adam'\n",
    "    resume = True\n",
    "    #weights = './pretrained/kitti/kitti_iter5.tar'\n",
    "    #weights = './COTR/out/default/checkpoint.pth.tar'\n",
    "    weights = None\n",
    "    rescale_rot = 0.5\n",
    "    rescale_transl = 0.5\n",
    "    precision = \"O0\"\n",
    "    norm = 'bn'\n",
    "    dropout = 0.0\n",
    "    max_depth = 80.\n",
    "    weight_point_cloud = 1000\n",
    "    log_frequency = 10\n",
    "    print_frequency = 50\n",
    "    starting_epoch = 0\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1, 2, 3'\n",
    "\n",
    "EPOCH = 1\n",
    "def _init_fn(worker_id, seed):\n",
    "    seed = seed + worker_id + EPOCH*100\n",
    "    print(f\"Init worker {worker_id} with seed {seed}\")\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def get_2D_lidar_projection(pcl, cam_intrinsic):\n",
    "    pcl_xyz = cam_intrinsic @ pcl.T\n",
    "    pcl_xyz = pcl_xyz.T\n",
    "    pcl_z = pcl_xyz[:, 2]\n",
    "    pcl_xyz = pcl_xyz / (pcl_xyz[:, 2, None] + 1e-10)\n",
    "    pcl_uv = pcl_xyz[:, :2]\n",
    "\n",
    "    return pcl_uv, pcl_z\n",
    "\n",
    "def lidar_project_depth(pc_rotated, cam_calib, img_shape):\n",
    "    pc_rotated = pc_rotated[:3, :].detach().cpu().numpy()\n",
    "    cam_intrinsic = cam_calib.numpy()\n",
    "    pcl_uv, pcl_z = get_2D_lidar_projection(pc_rotated.T, cam_intrinsic)\n",
    "    mask = (pcl_uv[:, 0] > 0) & (pcl_uv[:, 0] < img_shape[1]) & (pcl_uv[:, 1] > 0) & (\n",
    "            pcl_uv[:, 1] < img_shape[0]) & (pcl_z > 0)\n",
    "    pcl_uv = pcl_uv[mask]\n",
    "    pcl_z = pcl_z[mask]\n",
    "    pcl_uv = pcl_uv.astype(np.uint32)\n",
    "    pcl_z = pcl_z.reshape(-1, 1)\n",
    "    depth_img = np.zeros((img_shape[0], img_shape[1], 1))\n",
    "    depth_img[pcl_uv[:, 1], pcl_uv[:, 0]] = pcl_z\n",
    "    depth_img = torch.from_numpy(depth_img.astype(np.float32))\n",
    "    depth_img = depth_img.cuda()\n",
    "    depth_img = depth_img.permute(2, 0, 1)\n",
    "\n",
    "    return depth_img, pcl_uv\n",
    "\n",
    "def two_images_side_by_side(img_a, img_b):\n",
    "    assert img_a.shape == img_b.shape, f'{img_a.shape} vs {img_b.shape}'\n",
    "    assert img_a.dtype == img_b.dtype\n",
    "    b, h, w, c = img_a.shape\n",
    "    canvas = np.zeros((b, h, 2 * w, c), dtype=img_a.cpu().numpy().dtype)\n",
    "    canvas[:, :, 0 * w:1 * w, :] = img_a.cpu().numpy()\n",
    "    canvas[:, :, 1 * w:2 * w, :] = img_b.cpu().numpy()\n",
    "    #canvas[:, :, : , 0 * w:1 * w] = img_a.cpu().numpy()\n",
    "    #canvas[:, :, : , 1 * w:2 * w] = img_b.cpu().numpy()\n",
    "    return canvas\n",
    "\n",
    "def pointcloud_to_depth_map(pointcloud: np.ndarray, theta_res=150, phi_res=32, max_depth=50, phi_min_degrees=60,\n",
    "                            phi_max_degrees=100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        All params are set so they match default carla lidar settings\n",
    "    \"\"\"\n",
    "    assert pointcloud.shape[1] == 3, 'Must have (N, 3) shape'\n",
    "    assert len(pointcloud.shape) == 2, 'Must have (N, 3) shape'\n",
    "\n",
    "    xs = pointcloud[:, 0]\n",
    "    ys = pointcloud[:, 1]\n",
    "    zs = pointcloud[:, 2]\n",
    "\n",
    "    rs = np.sqrt(np.square(xs) + np.square(ys) + np.square(zs))\n",
    "\n",
    "    phi_min = np.deg2rad(phi_min_degrees)\n",
    "    phi_max = np.deg2rad(phi_max_degrees)\n",
    "    phi_range = phi_max - phi_min\n",
    "    phis = np.arccos(zs / rs)\n",
    "\n",
    "    THETA_MIN = -np.pi\n",
    "    THETA_MAX = np.pi\n",
    "    THETA_RANGE = THETA_MAX - THETA_MIN\n",
    "    thetas = np.arctan2(xs, ys)\n",
    "\n",
    "    phi_indices = ((phis - phi_min) / phi_range) * (phi_res - 1)\n",
    "    phi_indices = np.rint(phi_indices).astype(np.int16)\n",
    "\n",
    "    theta_indices = ((thetas - THETA_MIN) / THETA_RANGE) * theta_res\n",
    "    theta_indices = np.rint(theta_indices).astype(np.int16)\n",
    "    theta_indices[theta_indices == theta_res] = 0\n",
    "\n",
    "    normalized_r = rs / max_depth\n",
    "\n",
    "    canvas = np.ones(shape=(theta_res, phi_res), dtype=np.float32)\n",
    "    print ('theta_indices' ,theta_indices )\n",
    "    print ('phi_indices', phi_indices )\n",
    "    # We might need to filter out out-of-bound phi values, if min-max degrees doesnt match lidar settings\n",
    "    canvas[theta_indices, phi_indices] = normalized_r\n",
    "\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "smart-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN training\n",
    "@ex.capture\n",
    "def train(model, optimizer, nested_tensor_input, query_input, target_transl, target_rot, loss_fn, point_clouds, loss):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Run model\n",
    "    #transl_err, rot_err = model(rgb_img, refl_img)\n",
    "    transl_err, rot_err = model(nested_tensor_input, query_input)\n",
    "    \"\"\"\n",
    "    print(\"transl_err==========\",transl_err)\n",
    "    print(\"rot_err=============\",rot_err)\n",
    "    print(\"point_clouds=============\",point_clouds)\n",
    "    print(\"target_transl=============\",target_transl)\n",
    "    print(\"target_rot=============\",target_rot)\n",
    "    \"\"\"\n",
    "    if loss == 'points_distance' or loss == 'combined':\n",
    "        losses = loss_fn(point_clouds, target_transl, target_rot, transl_err, rot_err)\n",
    "    else:\n",
    "        losses = loss_fn(target_transl, target_rot, transl_err, rot_err)\n",
    "    \n",
    "    #print(\"losses=============\",losses)\n",
    "\n",
    "    #losses['total_loss'].backward()\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return losses, rot_err, transl_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "liable-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN test\n",
    "@ex.capture\n",
    "def val(model, nested_tensor_input, query_input, target_transl, target_rot, loss_fn, point_clouds, loss):\n",
    "    model.eval()\n",
    "\n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        #transl_err, rot_err = model(rgb_img, refl_img)\n",
    "        transl_err, rot_err = model(nested_tensor_input, query_input)\n",
    "\n",
    "    if loss == 'points_distance' or loss == 'combined':\n",
    "        losses = loss_fn(point_clouds, target_transl, target_rot, transl_err, rot_err)\n",
    "    else:\n",
    "        losses = loss_fn(target_transl, target_rot, transl_err, rot_err)\n",
    "\n",
    "    # if loss != 'points_distance':\n",
    "    #     total_loss = loss_fn(target_transl, target_rot, transl_err, rot_err)\n",
    "    # else:\n",
    "    #     total_loss = loss_fn(point_clouds, target_transl, target_rot, transl_err, rot_err)\n",
    "\n",
    "    total_trasl_error = torch.tensor(0.0).cuda()\n",
    "    target_transl = torch.tensor(target_transl).cuda()\n",
    "    #total_rot_error = quaternion_distance(target_rot, rot_err, target_rot.device)\n",
    "    #total_rot_error = total_rot_error * 180. / math.pi\n",
    "    for j in range(nested_tensor_input.shape[0]):\n",
    "        total_trasl_error += torch.norm(target_transl[j] - transl_err[j]) * 100.\n",
    "\n",
    "    # # output image: The overlay image of the input rgb image and the projected lidar pointcloud depth image\n",
    "    # cam_intrinsic = camera_model[0]\n",
    "    # rotated_point_cloud =\n",
    "    # R_predicted = quat2mat(R_predicted[0])\n",
    "    # T_predicted = tvector2mat(T_predicted[0])\n",
    "    # RT_predicted = torch.mm(T_predicted, R_predicted)\n",
    "    # rotated_point_cloud = rotate_forward(rotated_point_cloud, RT_predicted)\n",
    "\n",
    "    #return losses, total_trasl_error.item(), total_rot_error.sum().item(), rot_err, transl_err\n",
    "    return losses, total_trasl_error.item(), rot_err, transl_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prescription-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ex.automain\n",
    "@ex.main\n",
    "def main(_config, _run, seed):\n",
    "    global EPOCH\n",
    "    print('Loss Function Choice: {}'.format(_config['loss']))\n",
    "\n",
    "    if _config['val_sequence'] is None:\n",
    "        raise TypeError('val_sequences cannot be None')\n",
    "    else:\n",
    "        _config['val_sequence'] = f\"{_config['val_sequence']:02d}\"\n",
    "        print(\"Val Sequence: \", _config['val_sequence'])\n",
    "        dataset_class = DatasetLidarCameraKittiOdometry\n",
    "    img_shape = (384, 1280) # 네트워크의 입력 규모\n",
    "    input_size = (256, 512)\n",
    "    _config[\"checkpoints\"] = os.path.join(_config[\"checkpoints\"], _config['dataset'])\n",
    "\n",
    "    dataset_train = dataset_class(_config['data_folder'], max_r=_config['max_r'], max_t=_config['max_t'],\n",
    "                                  split='train', use_reflectance=_config['use_reflectance'],\n",
    "                                  val_sequence=_config['val_sequence'])\n",
    "    dataset_val = dataset_class(_config['data_folder'], max_r=_config['max_r'], max_t=_config['max_t'],\n",
    "                                split='val', use_reflectance=_config['use_reflectance'],\n",
    "                                val_sequence=_config['val_sequence'])\n",
    "    model_savepath = os.path.join(_config['checkpoints'], 'val_seq_' + _config['val_sequence'], 'models')\n",
    "    if not os.path.exists(model_savepath):\n",
    "        os.makedirs(model_savepath)\n",
    "    log_savepath = os.path.join(_config['checkpoints'], 'val_seq_' + _config['val_sequence'], 'log')\n",
    "    if not os.path.exists(log_savepath):\n",
    "        os.makedirs(log_savepath)\n",
    "    train_writer = SummaryWriter(os.path.join(log_savepath, 'train'))\n",
    "    val_writer = SummaryWriter(os.path.join(log_savepath, 'val'))\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "\n",
    "    def init_fn(x): return _init_fn(x, seed)\n",
    "\n",
    "    train_dataset_size = len(dataset_train)\n",
    "    val_dataset_size = len(dataset_val)\n",
    "    print('Number of the train dataset: {}'.format(train_dataset_size))\n",
    "    print('Number of the val dataset: {}'.format(val_dataset_size))\n",
    "\n",
    "    # Training and validation set creation\n",
    "    num_worker = _config['num_worker']\n",
    "    batch_size = _config['batch_size']\n",
    "    print(\"batch_size=\" , batch_size)\n",
    "    TrainImgLoader = torch.utils.data.DataLoader(dataset=dataset_train,\n",
    "                                                 shuffle=True,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 num_workers=num_worker,\n",
    "                                                 worker_init_fn=init_fn,\n",
    "                                                 collate_fn=merge_inputs,\n",
    "                                                 drop_last=False,\n",
    "                                                 pin_memory=True)\n",
    "\n",
    "    ValImgLoader = torch.utils.data.DataLoader(dataset=dataset_val,\n",
    "                                                shuffle=False,\n",
    "                                                batch_size=batch_size,\n",
    "                                                num_workers=num_worker,\n",
    "                                                worker_init_fn=init_fn,\n",
    "                                                collate_fn=merge_inputs,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True)\n",
    "\n",
    "    print(len(TrainImgLoader))\n",
    "    print(len(ValImgLoader))\n",
    "\n",
    "    # loss function choice\n",
    "    if _config['loss'] == 'simple':\n",
    "        loss_fn = ProposedLoss(_config['rescale_transl'], _config['rescale_rot'])\n",
    "    elif _config['loss'] == 'geometric':\n",
    "        loss_fn = GeometricLoss()\n",
    "        loss_fn = loss_fn.cuda()\n",
    "    elif _config['loss'] == 'points_distance':\n",
    "        loss_fn = DistancePoints3D()\n",
    "    elif _config['loss'] == 'L1':\n",
    "        loss_fn = L1Loss(_config['rescale_transl'], _config['rescale_rot'])\n",
    "    elif _config['loss'] == 'combined':\n",
    "        loss_fn = CombinedLoss(_config['rescale_transl'], _config['rescale_rot'], _config['weight_point_cloud'])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown Loss Function\")\n",
    "\n",
    "    #runs = datetime.now().strftime('%b%d_%H-%M-%S') + \"/\"\n",
    "    # train_writer = SummaryWriter('./logs/' + runs)\n",
    "    #ex.info[\"tensorflow\"] = {}\n",
    "    #ex.info[\"tensorflow\"][\"logdirs\"] = ['./logs/' + runs]\n",
    "    \n",
    "    # Todo : 여기서 부터 잘 고쳐야함 !!!\n",
    "    # network choice and settings\n",
    "    if _config['network'].startswith('Res'):\n",
    "        feat = 1\n",
    "        md = 4\n",
    "        split = _config['network'].split('_')\n",
    "        for item in split[1:]:\n",
    "            if item.startswith('f'):\n",
    "                feat = int(item[-1])\n",
    "            elif item.startswith('md'):\n",
    "                md = int(item[2:])\n",
    "        assert 0 < feat < 7, \"Feature Number from PWC have to be between 1 and 6\"\n",
    "        assert 0 < md, \"md must be positive\"\n",
    "        model = LCCNet(input_size, use_feat_from=feat, md=md,\n",
    "                         use_reflectance=_config['use_reflectance'], dropout=_config['dropout'],\n",
    "                         Action_Func='leakyrelu', attention=False, res_num=18)\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\"Network unknown\")\n",
    "    if _config['weights'] is not None:\n",
    "        print(f\"Loading weights from {_config['weights']}\")\n",
    "        checkpoint = torch.load(_config['weights'], map_location='cpu')\n",
    "        print('-------checkpoint-keys-------',checkpoint.keys() )\n",
    "        saved_state_dict = checkpoint['model_state_dict']\n",
    "        model.load_state_dict(saved_state_dict)\n",
    "\n",
    "        # original saved file with DataParallel\n",
    "        # state_dict = torch.load(model_path)\n",
    "        # create new OrderedDict that does not contain `module.`\n",
    "        # from collections import OrderedDict\n",
    "        # new_state_dict = OrderedDict()\n",
    "        # for k, v in checkpoint['state_dict'].items():\n",
    "        #     name = k[7:]  # remove `module.`\n",
    "        #     new_state_dict[name] = v\n",
    "        # # load params\n",
    "        # model.load_state_dict(new_state_dict)\n",
    "\n",
    "    # model = model.to(device)\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.cuda()\n",
    "\n",
    "    print('Number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n",
    "\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    if _config['loss'] == 'geometric':\n",
    "        parameters += list(loss_fn.parameters())\n",
    "    if _config['optimizer'] == 'adam':\n",
    "        optimizer = optim.Adam(parameters, lr=_config['BASE_LEARNING_RATE'], weight_decay=5e-6)\n",
    "        # Probably this scheduler is not used\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 50, 70], gamma=0.5)\n",
    "    else:\n",
    "        optimizer = optim.SGD(parameters, lr=_config['BASE_LEARNING_RATE'], momentum=0.9,\n",
    "                              weight_decay=5e-6, nesterov=True)\n",
    "    \n",
    "    starting_epoch = _config['starting_epoch']\n",
    "    if _config['weights'] is not None and _config['resume']:\n",
    "        checkpoint = torch.load(_config['weights'], map_location='cpu')\n",
    "        opt_state_dict = checkpoint['optimizer']\n",
    "        optimizer.load_state_dict(opt_state_dict)\n",
    "        if starting_epoch != 0:\n",
    "            starting_epoch = checkpoint['epoch']\n",
    "\n",
    "    # Allow mixed-precision if needed\n",
    "    # model, optimizer = apex.amp.initialize(model, optimizer, opt_level=_config[\"precision\"])\n",
    "\n",
    "    start_full_time = time.time()\n",
    "    BEST_VAL_LOSS = 10000.\n",
    "    old_save_filename = None\n",
    "\n",
    "    train_iter = 0\n",
    "    val_iter = 0\n",
    "#     for epoch in range(starting_epoch, _config['epochs'] + 1):\n",
    "#         EPOCH = epoch\n",
    "#         print('This is %d-th epoch' % epoch)\n",
    "#         epoch_start_time = time.time()\n",
    "#         total_train_loss = 0\n",
    "#         local_loss = 0.\n",
    "#         if _config['optimizer'] != 'adam':\n",
    "#             _run.log_scalar(\"LR\", _config['BASE_LEARNING_RATE'] *\n",
    "#                             math.exp((1 - epoch) * 4e-2), epoch)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = _config['BASE_LEARNING_RATE'] * \\\n",
    "#                                     math.exp((1 - epoch) * 4e-2)\n",
    "#         else:\n",
    "#             #scheduler.step(epoch%100)\n",
    "#             _run.log_scalar(\"LR\", scheduler.get_lr()[0])\n",
    "        \n",
    "\n",
    "        ## Training ##\n",
    "    time_for_50ep = time.time()\n",
    "    # 실질적인 물리적인 의미의 data input preprocessing 작업\n",
    "    for batch_idx, sample in enumerate(TrainImgLoader):\n",
    "        #print(f'batch {batch_idx+1}/{len(TrainImgLoader)}', end='\\r')\n",
    "        start_time = time.time()\n",
    "        lidar_input = []\n",
    "        rgb_input = []\n",
    "        lidar_gt = []\n",
    "        shape_pad_input = []\n",
    "        real_shape_input = []\n",
    "        pc_rotated_input = []\n",
    "\n",
    "        # gt pose\n",
    "        sample['tr_error'] = sample['tr_error'].cuda()\n",
    "        sample['rot_error'] = sample['rot_error'].cuda()\n",
    "\n",
    "        start_preprocess = time.time()\n",
    "        for idx in range(len(sample['rgb'])):\n",
    "            # ProjectPointCloud in RT-pose\n",
    "            real_shape = [sample['rgb'][idx].shape[1], sample['rgb'][idx].shape[2], sample['rgb'][idx].shape[0]]\n",
    "            \n",
    "            # depth map dispaly\n",
    "            \"\"\"\n",
    "            pointcloud = sample['point_cloud'][idx].clone()\n",
    "            print('-------pointcloud.shape-------------' , pointcloud.shape)\n",
    "            pointcloudT = pointcloud.T\n",
    "            print('-------pointcloudTranspose.shape-------------' , pointcloudT.shape)\n",
    "            pointcloud1= pointcloudT[:,:3]\n",
    "            print('-------pointcloud1.shape-------------' , pointcloud1.shape)\n",
    "            pointcloud2=pointcloud1.cpu().numpy()\n",
    "            \n",
    "            \n",
    "            \n",
    "            depth_map = pointcloud_to_depth_map(pointcloud2)\n",
    "            depth_map = depth_map * 256\n",
    "            depth_map = np.flip(depth_map, axis=1) # so floor is down\n",
    "            depth_map = np.swapaxes(depth_map, 0, 1)\n",
    "            print('----------- depthmap display--------------')\n",
    "            plt.imshow(depth_map, cmap='gray_r')\n",
    "            plt.show()\n",
    "            \"\"\"\n",
    "            \n",
    "            sample['point_cloud'][idx] = sample['point_cloud'][idx].cuda() # 카메라 좌표계로 변환 된 Lidar 포인트 클라우드\n",
    "            pc_lidar = sample['point_cloud'][idx].clone()\n",
    "\n",
    "            if _config['max_depth'] < 80.:\n",
    "                pc_lidar = pc_lidar[:, pc_lidar[0, :] < _config['max_depth']].clone()\n",
    "\n",
    "            depth_gt, uv = lidar_project_depth(pc_lidar, sample['calib'][idx], real_shape) # image_shape\n",
    "            depth_gt /= _config['max_depth']\n",
    "\n",
    "            R = mathutils.Quaternion(sample['rot_error'][idx]).to_matrix()\n",
    "            R.resize_4x4()\n",
    "            T = mathutils.Matrix.Translation(sample['tr_error'][idx])\n",
    "            #RT = T * R\n",
    "            RT = T @ R # version change matutils * --> @ \n",
    "\n",
    "            pc_rotated = rotate_back(sample['point_cloud'][idx], RT) # Pc` = RT * Pc\n",
    "\n",
    "            if _config['max_depth'] < 80.:\n",
    "                pc_rotated = pc_rotated[:, pc_rotated[0, :] < _config['max_depth']].clone()\n",
    "\n",
    "            depth_img, uv = lidar_project_depth(pc_rotated, sample['calib'][idx], real_shape) # image_shape\n",
    "            depth_img /= _config['max_depth']\n",
    "\n",
    "            # PAD ONLY ON RIGHT AND BOTTOM SIDE\n",
    "            rgb = sample['rgb'][idx].cuda()\n",
    "            shape_pad = [0, 0, 0, 0]\n",
    "\n",
    "            shape_pad[3] = (img_shape[0] - rgb.shape[1])  # // 2\n",
    "            shape_pad[1] = (img_shape[1] - rgb.shape[2])  # // 2 + 1\n",
    "\n",
    "            rgb = F.pad(rgb, shape_pad)\n",
    "            depth_img = F.pad(depth_img, shape_pad)\n",
    "            depth_gt = F.pad(depth_gt, shape_pad)\n",
    "\n",
    "            rgb_input.append(rgb)\n",
    "            lidar_input.append(depth_img)\n",
    "            lidar_gt.append(depth_gt)\n",
    "            real_shape_input.append(real_shape)\n",
    "            shape_pad_input.append(shape_pad)\n",
    "            pc_rotated_input.append(pc_rotated)\n",
    "\n",
    "        lidar_input = torch.stack(lidar_input)\n",
    "        rgb_input = torch.stack(rgb_input)\n",
    "        rgb_show = rgb_input.clone()\n",
    "        lidar_show = lidar_input.clone()\n",
    "        #rgb_input = F.interpolate(rgb_input, size=[256, 516], mode=\"bilinear\")\n",
    "        #lidar_input = F.interpolate(lidar_input, size=[256, 512], mode=\"bilinear\")\n",
    "        rgb_resize = F.interpolate(rgb_input, size=[256, 256], mode=\"bilinear\")\n",
    "        lidar_resize = F.interpolate(lidar_input, size=[256, 256], mode=\"bilinear\")\n",
    "        #print (\"---------rgb_input_shape--------\", rgb_input.shape)\n",
    "        #print (\"---------lidar_input_shape--------\", lidar_input.shape)\n",
    "\n",
    "        lidar_input_nested = torch.cat([lidar_resize,lidar_resize,lidar_resize],1)\n",
    "        nested_input = two_images_side_by_side(rgb_resize, lidar_input_nested)\n",
    "        nested_input = torch.from_numpy(nested_input).cuda()\n",
    "        #print (\"---------nested_input_raw_shape--------\", nested_input.shape)\n",
    "        nested_input = nested_input.permute(0,1,3,2)\n",
    "        #print (\"---------nested_input_shape--------\", nested_input.shape)\n",
    "        nested_input_squeeze = torch.squeeze(nested_input)\n",
    "        nested_input_squeeze = nested_input_squeeze.permute(1,2,0)\n",
    "        #print (\"---------nested_input_squeeze_shape--------\", nested_input_squeeze.shape)\n",
    "        sample_ndarray = nested_input_squeeze.cpu().numpy()\n",
    "        sample_ndarray = tvtf.normalize(tvtf.to_tensor(sample_ndarray), (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)).float()[None].cuda()\n",
    "        #print (\"---------sample_ndarray_shape--------\", sample_ndarray.shape)\n",
    "\n",
    "        q_list = []\n",
    "        for i in range(256):\n",
    "            queries = []\n",
    "            for j in range(256 * 2):\n",
    "                queries.append([(j) / (256 * 2), i / 256])\n",
    "            queries = np.array(queries)\n",
    "            q_list.append(queries)\n",
    "        q_list = torch.tensor(q_list).float().cuda()\n",
    "        #queries = torch.from_numpy(np.concatenate(q_list))[None].float().cuda()\n",
    "\n",
    "        \"\"\"\n",
    "        print (\"---------rgb_input_shape--------\", rgb_input.shape)\n",
    "        print (\"---------lidar_input_shape--------\", lidar_input.shape)\n",
    "        print (\"---------lidar_input_shape--------\", lidar_input_nested.shape)\n",
    "        print (\"---------nested_input_shape--------\", nested_input.shape)\n",
    "        print (\"---------queries_shape--------\", queries.shape)\n",
    "        \n",
    "        \n",
    "        lidar_input_cv2 = torch.squeeze(lidar_input ,dim=0)\n",
    "        rgb_input_cv2 = torch.squeeze(rgb_input)\n",
    "        print (\"---------lidar_input_shape--------\", lidar_input_cv2.shape)\n",
    "        print (\"---------rgb_input_shape--------\", rgb_input_cv2.shape)\n",
    "        lidar_input_cv2 = lidar_input_cv2.permute(1,2,0)\n",
    "        rgb_input_cv2 = rgb_input_cv2.permute(2,1,0)\n",
    "        lidar_input_cv2 = lidar_input_cv2.cpu().numpy()\n",
    "        rgb_input_cv2 = rgb_input_cv2.cpu().numpy()\n",
    "        \"\"\"\n",
    "        \n",
    "#         print (\"---------lidar_input_shape--------\", lidar_input.shape)\n",
    "#         print (\"---------rgb_input_shape--------\", rgb_input.shape)\n",
    "        #cv2.imshow(lidar_input_cv2 , cmap='gray')\n",
    "        \n",
    "        out0 , rgb_show1 , lidar_raw_show = overlay_imgs(rgb_input[0], lidar_input)\n",
    "        out1 , rgb_show2 , lidar_gt_show = overlay_imgs(rgb_input[0], lidar_gt[0].unsqueeze(0))\n",
    "        \"\"\"\n",
    "        #CV2 display\n",
    "        cv2.imshow(\"INPUT\", out0[:, :, [2, 1, 0]])\n",
    "        cv2.imshow(\"GT\", out1[:, :, [2, 1, 0]])\n",
    "        cv2.waitKey(1)\n",
    "        \"\"\"\n",
    "        \n",
    "        blended_image=out0[:, :, [2, 1, 0]]\n",
    "        blended_image1=out1[:, :, [2, 1, 0]]\n",
    "        \n",
    "        sift = cv2.xfeatures2d.SIFT_create()\n",
    "        gftt = cv2.GFTTDetector_create()\n",
    "        \n",
    "        rgb_show2 = rgb_show1.astype('uint8')\n",
    "        lidar_gt_show1 = lidar_gt_show.astype('uint8')\n",
    "        \n",
    "        cv2.imwrite(\"blended_image.jpg\", blended_image)\n",
    "        \n",
    "        rgb_show1_gray = cv2.cvtColor(rgb_show2, cv2.COLOR_BGR2GRAY)\n",
    "        lidar_gt_show_gray = cv2.cvtColor(lidar_gt_show1, cv2.COLOR_BGR2GRAY)\n",
    "        rgb_show3 = rgb_show1_gray\n",
    "        \n",
    "        #kp1, des1 = sift.detectAndCompute(rgb_show2,None)\n",
    "        #kp2, des2 = sift.detectAndCompute(lidar_gt_show1,None)\n",
    "        kp1 = gftt.detect(rgb_show1_gray ,None)\n",
    "        kp2 = gftt.detect(lidar_gt_show1,None)\n",
    "\n",
    "        #draw keypoints\n",
    "        rgb_show1_keypoints = cv2.drawKeypoints(rgb_show1_gray , kp1 , outImage=np.array([]))\n",
    "        #print ('---------------------------rgb_show1_keypoints---------------------')\n",
    "        #print (kp1)\n",
    "        #print ('-------------------------------------------------------------------')\n",
    "        lidar_gt_show_keypoints = cv2.drawKeypoints(lidar_gt_show_gray , kp2 , outImage=np.array([]))\n",
    "        \n",
    "        # BFMatcher with default params\n",
    "        bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
    "        #matches = bf.knnMatch(des1,des2, k=2)\n",
    "        #matches = sorted(matches, key = lambda x:x.distance)       \n",
    "        \n",
    "        # Apply ratio test\n",
    "#         good = []\n",
    "#         for m,n in matches:\n",
    "#             if m.distance < 0.75*n.distance:\n",
    "#                 good.append([m])\n",
    "\n",
    "        #rgb_show3 =cv2.drawMatches(rgb_show1_gray, kp1, lidar_gt_show_gray , kp2 , outImage=np.array([]) , color=(255,0,0))\n",
    "        #rgb_show3 =cv2.drawMatchesKnn(rgb_show1_gray, kp1, lidar_gt_show_gray , kp2 , matches[:50] , rgb_show3 , flags=2)\n",
    "        \n",
    "        \n",
    "        print (\"---------rgb_show1--------\", rgb_show1.shape)\n",
    "        plt.imshow(rgb_show1)\n",
    "        plt.show()\n",
    "        #im1=Image(rgb_show1)\n",
    "        #im1.save(rgb_show1.jpg)\n",
    "        #mpimg.imsave(\"rgb_show1.png\", rgb_show1)\n",
    "        #plt.savefig('rgb_show1.png')\n",
    "        print (\"---------rgb_show1_gray--------\", rgb_show1_gray.shape)\n",
    "        plt.imshow(rgb_show1_gray , cmap='gray')\n",
    "        plt.show()\n",
    "        print (\"---------rgb_show1_keypoints--------\", rgb_show1_keypoints.shape)\n",
    "        plt.imshow(rgb_show1_keypoints , cmap='gray')\n",
    "        plt.show() \n",
    "        print (\"---------lidar_gt_show_keypoints--------\", lidar_gt_show_keypoints.shape)\n",
    "        plt.imshow(lidar_gt_show_keypoints , cmap='gray')\n",
    "        plt.show()\n",
    "        print (\"---------lidar_raw_show--------\", lidar_raw_show.shape)\n",
    "        plt.imshow(lidar_raw_show)\n",
    "        plt.show()\n",
    "        print (\"---------lidar_gt_show--------\", lidar_gt_show.shape)\n",
    "        plt.imshow(lidar_gt_show)\n",
    "        plt.show()\n",
    "        #mpimg.imsave(\"lidar_gt_show.png\", lidar_gt_show)\n",
    "        #plt.savefig('lidar_gt_show.png')\n",
    "        print (\"---------lidar_gt_show_gray--------\", lidar_gt_show_gray.shape)\n",
    "        plt.imshow(lidar_gt_show_gray , cmap='gray')\n",
    "        plt.show()\n",
    "        print (\"---------raw_image--------\", blended_image.shape)\n",
    "        plt.imshow(blended_image)\n",
    "        plt.show()\n",
    "        print (\"---------gt_image--------\", blended_image1.shape)\n",
    "        plt.imshow(blended_image1)\n",
    "        plt.show()\n",
    "        print (\"---------KeyPointDisplay--------\", rgb_show3.shape)\n",
    "        plt.imshow(rgb_show3)\n",
    "        plt.show()\n",
    "        \n",
    "        end_preprocess = time.time()\n",
    "\n",
    "    print('full training time = %.2f HR' % ((time.time() - start_full_time) / 3600))\n",
    "    return _run.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "approximate-progress",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 2022-03-31 14:08:33,041 - run - No observers have been added to this run\n",
      "INFO - 2022-03-31 14:08:33,055 - run - Running command 'main'\n",
      "INFO - 2022-03-31 14:08:33,059 - run - Started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Function Choice: combined\n",
      "Val Sequence:  00\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 11.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 12.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 13.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 14.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 15.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 16.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 17.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 18.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 19.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 20.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 21.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 11.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 12.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 13.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 14.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 15.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 16.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 17.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 18.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 19.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 20.\n",
      "-pose_path- /mnt/data/kitti_odometry/data_odometry_poses/dataset/poses\n",
      "Ground truth poses are not avaialble for sequence 21.\n",
      "VAL SET - Not found: /mnt/data/kitti_odometry/data_odometry_valRT/dataset/sequences/val_RT_left_seq00_10.00_1.00.csv\n",
      "Generating a new one\n",
      "Number of the train dataset: 39011\n",
      "Number of the val dataset: 4541\n",
      "batch_size= 1\n",
      "39011\n",
      "4541\n",
      "using lin_sine as positional encoding\n",
      "using lin_sine as positional encoding\n",
      "Loading weights from ./COTR/out/default/checkpoint.pth.tar\n",
      "weights safely loaded\n",
      "COTR pre-trained weights freeze\n",
      "Number of model parameters: 18783689\n",
      "Init worker 0 with seed 132719062\n",
      "Init worker 1 with seed 132719063\n",
      "Init worker 2 with seed 132719064\n",
      "Init worker 3 with seed 132719065\n",
      "Init worker 4 with seed 132719066\n",
      "Init worker 5 with seed 132719067\n",
      "Init worker 6 with seed 132719068\n",
      "Init worker 7 with seed 132719069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR - 2022-03-31 14:08:54,467 - run - Failed after 0:00:21!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/root/venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/root/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/root/work/LCCNet_Moon/utils.py\", line 129, in merge_inputs\n    corrs.append(input['corrs'])\nKeyError: 'corrs'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-106ea74a54b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/sacred/experiment.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, command_name, config_updates, named_configs, info, meta_info, options)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mcommand_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_updates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamed_configs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         )\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/sacred/run.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_heartbeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_pre_run_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_post_run_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/sacred/config/captured_function.py\u001b[0m in \u001b[0;36mcaptured_function\u001b[0;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# =================== run actual function =================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mConfigError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;31m# =========================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c1771b27620a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_config, _run, seed)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mtime_for_50ep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# 실질적인 물리적인 의미의 data input preprocessing 작업\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainImgLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;31m#print(f'batch {batch_idx+1}/{len(TrainImgLoader)}', end='\\r')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/root/venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/root/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/root/work/LCCNet_Moon/utils.py\", line 129, in merge_inputs\n    corrs.append(input['corrs'])\nKeyError: 'corrs'\n"
     ]
    }
   ],
   "source": [
    "ex.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
